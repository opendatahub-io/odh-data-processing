# teigaku_genzei
A sample data of teigaku genzei (the fixed-amount tax reduction) for InstructLab / sdg_hub seed generation and testing.

# Data source

This example is based on the topic of Teigaku Genzei (the fixed-amount tax reduction) of Reiwa 6 (2024) by National Tax Agency (NTA) of Japan.
- https://www.nta.go.jp/users/gensen/teigakugenzei/01.htm

In particular, the following FAQ documents are used.
- https://www.nta.go.jp/publication/pamph/gensen/0024001-021.pdf
- https://www.nta.go.jp/publication/pamph/shotoku/0024004-072_01.pdf

In this example, the first file is used for creating training and testing datasets. 
The second file is used for making in-context learning examples for SDG (synthetic data generation) of training data, i.e., used for teaching the teacher model how to synthesize the training data.
These datasets will be produced by converter scripts provided in this repository with helps from Docling and SDG hub.
These datasets as well as the fine-tuned model trained with the training data and its outputs to users
may contain information derived from the above Teigaku Genzei documents with modifications by the converters and LLMs.

**IMPORTANT**: NTA's Teigaku Genzei documents are distributed and licensed to each user by NTA under the following terms and conditions.
- Âà©Áî®Ë¶èÁ¥Ñ„ÉªÂÖçË≤¨‰∫ãÈ†Ö„ÉªËëó‰ΩúÊ®© (JA) https://www.nta.go.jp/chuijiko/copy.htm
- Terms of Use, Disclaimer, Copyright (EN) https://www.nta.go.jp/english/notice/index.htm

Neither Red Hat nor IBM is a licensee or licensor of such NTA's Teigaku Genzei documents.

<!-- - IBM granite-3.3-8b-instruct did not use training data that is derived from these NTA's documents. -->
<!-- - For example, if the fine-tuned model is used in a Tax QA system, then one should ensure that QA system displays the link to the source of the data and the existence of the modification to the data, etc. to all the end users of the QA system. -->

Please download these two PDF files by yourself.


# Common preprocessing of source document

## Prerequisites
- teigaku_genzei (this repository)
    - Clone this repository and locate it in your work directory.
- sdg_hub
    - GitHub - Red-Hat-AI-Innovation-Team/sdg_hub: Synthetic Data Generation Toolkit for LLMs
    - https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub
    - Install sdg_hub tool by following README of sdg_hub.
    - In particular, you need `docparser_v2.py` in  `examples/knowledge_tuning/instructlab/docparser_v2.py`. 
        - This is a wrapper of Docling.
## Procedure
- It is assumed that you are in the root directory of this repository, which is `teigaku_genzei`.
- Locate the source documents above (i.e., 0024001-021.pdf and 0024004-072_01.pdf) to source/ directory.
- Run `docparser_v2.py` using the following command.
    - `./parser.sh`
    - You will see the texts extracted from the PDF are stored in `docparser/` directory.
    - Note: we assume that  `sdg_hub/` (the clone of the sdg_hub repository) exists next to `teigaku_genzei/`. If not, please fix `code_path` in `parser.sh` appropriately.
- Extract the QA pairs and the glossary from the JSON file in the above directory by the following command.
    - `./extract_qa.sh`
    - You will see that there are 2 x 2 = 4 CSV files in `qa_table/` directory. 
        - `{source_file_name}.csv` : QA pairs. Collumns are: `Title`, `Question`, `Answer`. These are used for constructing the SDG seed data as well as for the test data.
        - `{source_file_name}_glossary.csv` : Glossary. Columns are: `Term`, `Description`. These are only used for constructing the SDG seed data.

## Command line interface

`json_qa.py` is used to extract QA pairs from the above FAQ documents.
```
Usage: python json_qa.py <input_json_file> <output_csv_file>
```
where
- input_json_file : A JSON file generated by `docparser_v2.py` (Docling) as a result of the extraction of text from a PDF.
- output_csv_file : A CSV file which contains QA pairs. See ["Input file format"](#input-file-format).

`json_glossary.py` are used to extract a glossary from the above FAQ documents.
```
Usage: python script.py <input_json_file> <output_csv_file>
```
where
- input_json_file :  A JSON file generated by `docparser_v2.py` (Docling) as a result of the extraction of text from a PDF.
- output_csv_file : A CSV file which contains the glossary in the above FAQ document. Columns are: `Term`, `Description`. 


# Testing / evaluating model performance

## Prerequisites
- Complete the procedure in [Common preprocessing of source document](#common-preprocessing-of-source-document).
- LLMs are executed locally in this section. GPU and CUDA environment are recommended. Or, you can try smaller models if you have CPUs only.
    - TODO: Testing remote model will be supported in a future.
- Python packages in `requirements.txt`.
    - These packages are needed to run Unitxt and its extensions. See Unitxt repository for the detail.
        - "GitHub - IBM/unitxt: ü¶Ñ Unitxt is a Python library for enterprise-grade evaluation of AI performance, offering the world's largest catalog of tools and data for end-to-end AI benchmarking" https://github.com/IBM/unitxt
    - These packages can be installed in the same Python environment with the one for sdg_hub. I have not tested the case where the packages are installed in a separate environment, though it should work in that case too. 

## One-time setup
- Create `tmp/` directory in this repository. This directory is ignored by git. The evaluation results will be stored in this directory.
- Configure LLM-as-a-Judge module in `test_single.sh`.
    - This test script uses LLM-as-a-Judge. The judge model is assumed to be hosted in a remote API service. One needs to specify which model in which service provider to use by specifying `JUDGE_MODULE` in this script.
    - Available values of this variable can be found in the [catalog of Unitxt](https://www.unitxt.ai/en/latest/catalog/catalog.metrics.llm_as_judge.direct.__dir__.html). Select a service provider where you have an account, select a model, and specify its identifier in the variable. E.g., `JUDGE_MODULE="metrics.llm_as_judge.direct.aws.deepseek_v3"`.
        - Currently, it supports only `direct` judges in Unitxt. If you replace `direct` with `direct_positional_bias`, then an extended judge that reflects the result of the positional bias check in the global score file is used. This extension is done locally in this repository.
    - Set your API key accordingly in an environment variable. The environment variable name should be `{PROVIDER_NAME}_API_KEY`, where `{PROVIDER_NAME}` is the capitalized provider name such as ‚ÄúWATSONX‚Äù, ‚ÄúTOGETHER-AI‚Äù, ‚ÄúOPEN-AI‚Äù. The list of available provider names is found in [Explanation about CrossProviderInferenceEngine](https://www.unitxt.ai/en/latest/catalog/catalog.metrics.llm_as_judge.direct.aws.deepseek_v3.html#explanation-about-crossproviderinferenceengine)
        - Equivalently, you can set `{PROVIDER_NAME}_API_KEY` in `.env` file in the same directory. This file is ignored by git.
- (Optional) Additional setup for Unitxt local catalog. 
    - Usually, one does not need to do this step, as the latest local catalog has been shared already in this repository.
    - This is a maintenance script to import the configurations for various LLM API service providers and their models to the Unitxt local catalog directory (`local_catalog/`), so that the customized LLM-as-Judge metrics can be executed on those service providers.
    - If you upgrade Unitxt by yourself, then run the following script.
    - `python llm_as_judge_direct_provider_importer.py`

## Procedure
- To test the baseline model, run the following command.
    - `./test_single.sh ibm-granite/granite-3.3-8b-instruct`
    - In this case, the test program `test_qa.py` automatically downloads the model from Hugging Face and store it in your cache directory (typically, in `~/.cache/`).
    - The model is executed locally. It will take 20 minutes - 1 hour to complete.
    - You will see two CSV files in `tmp/single_{judge_module_name}__{tested_model_name}/` directory.
        - global_0024001-021_after.csv: The global score file. The aggregated scores over the data set are shown.
        - result_0024001-021_after.csv: The instance score file. Scores of each sample as well as the assessments (a long text) by the judge models are shown.
    - Caches of the inference outputs of the tested model as well as the judge model are stored in `inference_engine_cache/` directory. Please remove the caches if you need a fresh result.
- Alternatively, if you already have a checkpoint model in a local directory, you can specify the path of that checkpoint in the command line argument.
    - `./test_single.sh {path_to_the_checkpoint}/granite-3.3-8b-instruct-sft-checkpoint`

## Command line interface
The test and evaluation is implemented in `test_qa.py`.
This program is used to test a model with a zero-shot QA task.

```
python test_qa.py [-h] [--model_path MODEL_PATH] [--test_file TEST_FILE] [--test_language TEST_LANGUAGE] [--out_result_file OUT_RESULT_FILE] [--out_global_score_file OUT_GLOBAL_SCORE_FILE] [--judge_module JUDGE_MODULE] [--api_url API_URL] [--api_key API_KEY] [--server_type {hf,openai,vllm-remote}] [--data_classification {public,proprietary}] [--instructtion INSTRUCTTION]
```

options:
- `-h, --help` :           Show this help message and exit
- `--model_path MODEL_PATH` :  A path to a local checkpoint, or a Hugging Face model path, of the test target model. See ["Procedure"](#procedure-2) section above.
- `--test_file TEST_FILE` : See below.
- `--test_language TEST_LANGUAGE` : See below.
- `--out_result_file OUT_RESULT_FILE` : See below.
- `--out_global_score_file OUT_GLOBAL_SCORE_FILE` : See below.
- `--judge_module JUDGE_MODULE` : See ["One-time setup"](#one-time-setup) above.
- `--api_url API_URL` : The API endpoint URL. This is effective only when an API server is used (i.e., `--server_type` is not `hf`) to host the test target model. The default is `http://0.0.0.0:8000/v1`.
- `--api_key API_KEY` : The API key of the API server. This is effective only when an API server is used (i.e., `--server_type` is not `hf`) to host the test target model. This option overrides the API key specified in the environment variable. See `--server_type` option. See ["One-time setup"](#one-time-setup) above. 
- `--server_type {hf,openai,vllm-remote}` : The type of the server of the test target model.
    - `hf` : The model is executed locally using `HFAutoModelInferenceEngine` (i.e., Hugging Face Transformers library).
    - `openai` : The model is accessed via OpenAI API. The endpoint URL is specified with `--api_url` option. The model name is specified with `--model_path` option. Environment variable `OPENAI_API_KEY`, or `--api_key` option is used to configure its API key. Note that the environment variable might be shared with the API key for the LLM-as-a-Judge. See ["One-time setup"](#one-time-setup) above. Note that `--data_classification` option must be set to `public` to use this option. 
    - `vllm-remote` : The model is accessed via OpenAI API. The endpoint URL is specified with `--api_url` option. The model name is specified with `--model_path` option. Environment variable `VLLM-REMOTE_API_KEY`, or `--api_key` option is used to configure its API key. Note that the environment variable might be shared with the API key for the LLM-as-a-Judge. See ["One-time setup"](#one-time-setup) above. Note that `--data_classification` option must be set to `public` or `proprietary` to use this option.
- `--data_classification {public,proprietary}` : This option specifies the confidentiality of the test data specified in `--test_file`. This classification is used to avoid accidents such as confidential information being sent to a public LLM API service. The default is `proprietary`. Note that, in Unitxt, most of the public / commercial LLM service providers including the ones for test model and for LLM-as-a-Judge are configured to accept only `public` datasets. See also `--server_type` option. See also [unitxt.inference module ‚Äî Unitxt](https://www.unitxt.ai/en/latest/unitxt.inference.html) to see the accepted data classification of LLM services. See [Sensitive data in unitxt ‚Äî Unitxt](https://www.unitxt.ai/en/latest/docs/data_classification_policy.html) to learn more about the data classification.
- `--instructtion INSTRUCTTION` : The instruction of the prompt. The default is one of {"ja": "‰ª•‰∏ã„ÅÆË≥™Âïè„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ", "en": "Answer the following question."}, depending on the value of `--test_language` option.

See also `test_single.sh` as an example of the command line options.

### Input file format
The input data file is specified in `--test_file` option. This should be a CSV file of the following format.
```
Title,Question,Answer
"Q1","What is the capital of France?","Paris"
```
Notes
- `Title` field is currently unused for testing. 
- Currently, one needs to provide labeled QA test cases. I.e., each question should have one reference answer.
    - TODO: support 

You also need to specify `--test_language` to indicate the language of the input file. 
The available language names are listed in `mappers` of [Sacrebleu (Unitxt catalog)](https://www.unitxt.ai/en/latest/catalog/catalog.metrics.sacrebleu.html). The default is currently `ja`.

### Output file format
There are two output files.
- Global score file: This is specified by `--out_global_score_file` option. 
    - The output is a CSV file. The format of the output is as follows: 
        - `score_name,score,ci_low,ci_high`
    - LLM-as-a-judge metrics are explained in [catalog.metrics.llm_as_judge.direct.criteria](https://www.unitxt.ai/en/latest/catalog/catalog.metrics.llm_as_judge.direct.criteria.__dir__.html).
        - Metrics with `_positional_bias` are the scores judged with prompts where the options 
        (e.g., "Rate the prediction in 1, 2, or 3.") are shown to the judge model in the reversed order
        (i.e., "Rate the prediction in 3, 2, or 1.").
        - Metrics with `_average` are the averages of the metrics with and without `_positional_bias`.
    - Sacrebleu metrics (`bp, ref_len, sacrebleu, sys_len`) are explained in [SacreBLEU - a Hugging Face Space by evaluate-metric](https://huggingface.co/spaces/evaluate-metric/sacrebleu).
    - Rouge metrics (`rouge1, rouge2, rougeL, rougeLsum`) are explained in [ROUGE - a Hugging Face Space by evaluate-metric](https://huggingface.co/spaces/evaluate-metric/rouge)
- Instance result file: This is specified by `--out_result_file` option.
    - The output is a CSV file.
    - Each row of the output file corresponds to a test instance.
    - Each column corresponds to an instance-level metric or intermediate output to compute a global metric.

# SDG seed generation

## Prerequisites
- Complete the procedure in [Common preprocessing of source document](#common-preprocessing-of-source-document).
- It is assumed that the Python environment for sdg_hub is used in this section.


## Procedure
- Create `tmp/` directory if it does not exist.
- Compose the seed context data.
    - `./make_context.sh`
    - This script generates two files in `context/*_context.csv`. This type of files contains a set of text chunks, which is generated from the QA pairs and the glossary. These chunks are used as the contexts of the SDG in the later steps.
    - Note that we eliminate all the questions in the QA pairs and construct the contexts from the answers only. This is because questions included in the seed context might let the teacher model to generate exactly the same questions for training, which can lead to the overfitting to the test case.
- Generate ICL examples from the other FAQ document, which is 0024004-072_01.pdf.
    - `./make_icl.sh`
    - This program stores the extracted ICL examples in `icl/icl.json`. See ["Command line interface"](#command-line-interface-2) for the detail of the file format.
    - This is an example to tell the teacher model how to generate QA pairs from a context in SDG, but not directly used in the training of a model.
    - It should be OK to prepare ICL examples manually or any other ways as long as it has the above file format and the ICL documents and query - response pairs are not exactly the same with the seed contexts, which is explained in the next step.
- Compose SDG seed data.
    - `./make_seed.sh`
    - This script creates `tmp/seed_ja.jsonl`. This file can be used for SDG.
    - If there are N ICL examples and M seed context, then the resulting JSONL file contains N x M seed samples, each of which is a combination of an ICL example and a seed context. That is, it tries to exhaust all the different combinations of ICL examples and seed contexts.

## Command line interface

`make_context.py` is used to make context data from QAs and glossaries. This program is specific to the Teigaku Genzei FAQ documents. This program is called from `make_context.sh`.
```
usage: make_context.py [-h] [--glossary_option {appendix,header,none}] [--qa_file input_qa.csv] [--glossary_file input_glossary.csv] [--out_context_file output_context.csv]
```
options:
-   `-h, --help` :            show this help message and exit
-   `--glossary_option {appendix,header,none}` : `appendix` - The glossary is added as one chunk (context). `header` - The glossary is concatenated with each section (context). `none` - The glossary is not used.
-   `--qa_file input_qa.csv` : Input Question - Answer pair file. See ["Input file format"](#input-file-format). Here, we assume that `Title` column contains a section number of each QA pair.
-   `--glossary_file input_glossary.csv` : Input glossary file. The columns are: `Term`, `Description`. 
-   `--out_context_file output_context.csv` : Output file. The columns are: `section,qindex,qlist,context`.
    The generated context should contain enough information to answer the user questions.
    In this case, it is a concatenation of the answers of the QA pairs that belong to the same section.


`make_icl.py` is used to make in-context learning examples from QAs and glossaries for the teacher model of SDG.
This program is specific to the Teigaku Genzei FAQ documents. This program is called from `make_icl.sh`.
```
usage: make_icl.py [-h] [--glossary_option {appendix,header,none}] [--short_context SHORT_CONTEXT] [--qa_file input_qa.csv] [--glossary_file input_glossary.csv] [--context_file input_context.csv] [--out_icl_file output_icl.jsonl]
```
options:
- `-h, --help` :            show this help message and exit
- `--glossary_option {appendix,header,none}` : Currently not used.
- `--short_context SHORT_CONTEXT` : A boolean option {True, False}. If True, the context of an ICL example (i.e., `icl_document`) becomes shorter than one section. The context only contains texts that are enough to generate three ICL questions. Otherwise, one context is one section. 
- `--qa_file input_qa.csv` Input Question - Answer pair file. See ["Input file format"](#input-file-format) above.
- `--glossary_file input_glossary.csv` : Input glossary file. The columns are: `Term`, `Description`. 
- `--context_file input_context.csv` : Input context file. See the description of `make_context.py` above.
- `--out_icl_file output_icl.jsonl` : Output ICL example file. Each line of the file has the following format:
    - `{"icl_document": "...", "icl_query_1": "....", "icl_response_1": "...", "icl_query_2": "...", "icl_response_2": "...", "icl_query_3": "...", "icl_response_3": "..."}`
    - I.e., it contains one document (so called "context") and three query - response pairs that are related to the context. 

`make_seed.py` is used to make a seed sample file from contexts and ICL samples for the teacher model of SDG.
This program is called from `make_seed.sh`.
```
usage: make_seed.py [-h] [--context_file CONTEXT_FILE] [--icl_file ICL_FILE] [--out_seed_file OUT_SEED_FILE] [--join_method {slide,cartesian}]
```
options:
- `-h, --help` :            show this help message and exit
- `--context_file CONTEXT_FILE` : Input context file. See the description of `make_context.py` above.
- `--icl_file ICL_FILE` : See the description of `make_icl.py` above.
- `--out_seed_file OUT_SEED_FILE` : Output seed file in JSONL format. The format is defined in sdg_hub.
- `--join_method {slide,cartesian}` : `slide` - Currently not supported. `cartesian` - Assign each ICL example to all of the contexts. See the description in ["Procedure"](#procedure-2) section above.


# Synthetic QA data generation for supervised fine-tuning
- See a sample notebook in sdg_hub:  `sdg_hub/examples/knowledge_tuning/instructlab/knowledge_generation_and_mixing.ipynb`.

