{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bbdb27",
   "metadata": {},
   "source": [
    "# Subset Selection Notebook 1: Data Preparation & Configuration\n",
    "\n",
    "## Overview\n",
    "This notebook is the first step in the **Subset Selection Pipeline**. It focuses on setting up the environment, configuring all necessary parameters, and preparing the input data for processing.\n",
    "\n",
    "## Purpose in Subset Selection\n",
    "Configuration and data preparation are critical foundation steps that enable efficient subset selection. This notebook:\n",
    "1. Sets up all parameters for data processing, encoding, templates, and system resources\n",
    "2. Loads and inspects input data to ensure it's properly formatted\n",
    "3. Establishes the DataProcessor class and utility functions used throughout the pipeline\n",
    "4. Automatically detects and configures available GPU resources for parallel processing\n",
    "\n",
    "## Output\n",
    "- **config**: ProcessingConfig object containing all pipeline parameters\n",
    "- **dataset**: Loaded and validated HuggingFace dataset\n",
    "- **processor**: DataProcessor instance ready for embedding generation\n",
    "- Used in Notebook 2 for embedding generation and Notebook 3 for subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38278505",
   "metadata": {},
   "source": [
    "## Introduction and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8471a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "%pip install datasets jinja2 tqdm h5py numpy torch transformers submodlib-py==0.0.3 nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc8a20",
   "metadata": {},
   "source": [
    "## Imports and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, TypedDict, TypeVar, Union\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Third Party Imports\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from jinja2 import BaseLoader, Environment\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Configure logging and warnings\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36b1d1",
   "metadata": {},
   "source": [
    "### Core Utility Functions\n",
    "\n",
    "These functions provide critical infrastructure for the embedding generation and subset selection pipeline:\n",
    "\n",
    "1. **`get_default_num_gpus(testing_mode)`**\n",
    "   - **Purpose**: Auto-detects GPUs for distributed embedding generation\n",
    "   - **Used in**: SystemConfig initialization, parallel encoding across GPUs\n",
    "   - **Behavior**: Returns number of available CUDA devices; falls back to CPU in testing mode\n",
    "   - **Error Handling**: Raises RuntimeError if no GPU found in production mode\n",
    "\n",
    "2. **`retry_on_exception(func)`**\n",
    "   - **Purpose**: Automatic retry with cleanup for transient GPU/computation errors\n",
    "   - **Used in**: Embedding generation and subset selection methods (Notebooks 2 & 3)\n",
    "   - **Handles**: GPU OOM errors, runtime errors, value/type/index errors\n",
    "   - **Recovery**: Cleans GPU memory and waits before retry (configurable delay)\n",
    "\n",
    "3. **`display_gpu_info()`**\n",
    "   - **Purpose**: Display detailed information about available GPU devices\n",
    "   - **Shows**: GPU count, device names, memory (total/allocated/free), current device\n",
    "   - **Useful for**: Debugging, resource planning, monitoring GPU usage\n",
    "   - **Returns**: Dictionary with GPU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def get_default_num_gpus(testing_mode: bool = False) -> int:\n",
    "    \"\"\"\n",
    "    Get the default number of GPUs based on available CUDA devices.\n",
    "    \n",
    "    Args:\n",
    "        testing_mode (bool): If True, allows CPU usage with warnings. For testing only.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        if testing_mode:\n",
    "            logger.warning(\n",
    "                \"No CUDA devices detected. Running in testing mode with CPU. \"\n",
    "                \"Production use requires GPU acceleration.\"\n",
    "            )\n",
    "            return 1\n",
    "        raise RuntimeError(\n",
    "            \"No CUDA devices detected. This functionality requires at least one GPU.\"\n",
    "        )\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "\n",
    "def retry_on_exception(func):\n",
    "    \"\"\"\n",
    "    Decorator to retry a function upon exception up to a maximum number of retries.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        last_exception = None\n",
    "        for attempt in range(self.config.system.max_retries):\n",
    "            try:\n",
    "                return func(self, *args, **kwargs)\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                last_exception = e\n",
    "                logger.error(f\"GPU out of memory on attempt {attempt + 1}: {str(e)}\")\n",
    "            except RuntimeError as e:\n",
    "                last_exception = e\n",
    "                logger.error(f\"PyTorch runtime error on attempt {attempt + 1}: {str(e)}\")\n",
    "            except ValueError as e:\n",
    "                last_exception = e\n",
    "                logger.error(f\"Value error on attempt {attempt + 1}: {str(e)}\")\n",
    "            except TypeError as e:\n",
    "                last_exception = e\n",
    "                logger.error(f\"Type error on attempt {attempt + 1}: {str(e)}\")\n",
    "            except IndexError as e:\n",
    "                last_exception = e\n",
    "                logger.error(f\"Index error on attempt {attempt + 1}: {str(e)}\")\n",
    "\n",
    "            if attempt < self.config.system.max_retries - 1:\n",
    "                logger.info(f\"Retrying in {self.config.system.retry_delay} seconds...\")\n",
    "                time.sleep(self.config.system.retry_delay)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        raise last_exception\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def display_gpu_info():\n",
    "    \"\"\"\n",
    "    Display detailed information about available GPU devices.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing GPU information\n",
    "    \"\"\"\n",
    "    gpu_info = {\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'gpu_count': 0,\n",
    "        'gpus': [],\n",
    "        'current_device': None\n",
    "    }\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🖥️  GPU Information\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"❌ CUDA is not available\")\n",
    "        print(\"💡 Running on CPU\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        return gpu_info\n",
    "    \n",
    "    gpu_info['gpu_count'] = torch.cuda.device_count()\n",
    "    gpu_info['current_device'] = torch.cuda.current_device()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🖥️  GPU Information\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✅ CUDA is available\")\n",
    "    print(f\"📊 Number of GPUs: {gpu_info['gpu_count']}\")\n",
    "    print(f\"🎯 Current GPU device: {gpu_info['current_device']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(gpu_info['gpu_count']):\n",
    "        device_props = torch.cuda.get_device_properties(i)\n",
    "        total_memory = device_props.total_memory / 1024**3  # Convert to GB\n",
    "        allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        free_memory = total_memory - reserved_memory\n",
    "        \n",
    "        gpu_data = {\n",
    "            'id': i,\n",
    "            'name': device_props.name,\n",
    "            'total_memory_gb': round(total_memory, 2),\n",
    "            'allocated_memory_gb': round(allocated_memory, 2),\n",
    "            'reserved_memory_gb': round(reserved_memory, 2),\n",
    "            'free_memory_gb': round(free_memory, 2),\n",
    "            'compute_capability': f\"{device_props.major}.{device_props.minor}\",\n",
    "            'multi_processor_count': device_props.multi_processor_count\n",
    "        }\n",
    "        gpu_info['gpus'].append(gpu_data)\n",
    "        \n",
    "        marker = \"🎯\" if i == gpu_info['current_device'] else \"  \"\n",
    "        print(f\"\\n{marker} GPU {i}: {device_props.name}\")\n",
    "        print(f\"   • Compute Capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"   • Multi-processors: {device_props.multi_processor_count}\")\n",
    "        print(f\"   • Total Memory: {total_memory:.2f} GB\")\n",
    "        print(f\"   • Allocated: {allocated_memory:.2f} GB\")\n",
    "        print(f\"   • Reserved: {reserved_memory:.2f} GB\")\n",
    "        print(f\"   • Free: {free_memory:.2f} GB ({(free_memory/total_memory)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    return gpu_info\n",
    "\n",
    "gpu_info = display_gpu_info()\n",
    "print(\"✅ Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a34f45",
   "metadata": {},
   "source": [
    "## Configuration Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd20f6",
   "metadata": {},
   "source": [
    "### BasicConfig Class\n",
    "\n",
    "Defines basic processing parameters with validation and helpful metadata.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `output_dir`: Directory where results will be saved\n",
    "- `batch_size`: Number of samples processed per batch (default: 100K for efficiency)\n",
    "- `num_folds`: Number of folds for cross-validation in subset selection\n",
    "- `combine_files`: Whether to merge multiple input files into one dataset\n",
    "- `epsilon`: Optimization parameter for submodular facility location\n",
    "  - Default: 160.0 (optimized for datasets >100K samples)\n",
    "  - For smaller datasets: use values starting from 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235619ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BasicConfig:\n",
    "    \"\"\"Basic configuration parameters\"\"\"\n",
    "    output_dir: str = \"../../assets/subset-selection/outputs\" # change this to your desired output directory\n",
    "    batch_size: int = 100000\n",
    "    num_folds: int = 50\n",
    "    combine_files: bool = False\n",
    "    epsilon: float = field(\n",
    "        default=160.0,\n",
    "        metadata={\n",
    "            \"advanced\": True,\n",
    "            \"help\": \"Epsilon parameter for the LazierThanLazyGreedy optimizer in facility location maximization. \"\n",
    "            \"Default of 160.0 is optimized for datasets >100k samples. \"\n",
    "            \"For smaller datasets, consider using much smaller values (starting from 0.1).\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization\"\"\"\n",
    "        if not 0 < self.epsilon <= 160:\n",
    "            raise ValueError(\"epsilon must be between 0 and 160\")\n",
    "\n",
    "    def validate_epsilon_for_dataset_size(self, dataset_size: int)->None:\n",
    "        \"\"\"\n",
    "        Validate epsilon parameter based on dataset size and provide appropriate warnings.\n",
    "\n",
    "        Args:\n",
    "            dataset_size (int): Size of the dataset being processed\n",
    "        \"\"\"\n",
    "        if dataset_size < 100000:\n",
    "            logger.warning(\n",
    "                \"Subset selection is highly recommended to be used only with dataset sizes over 100k samples. \"\n",
    "                f\"Your dataset has {dataset_size:,} samples.\"\n",
    "            )\n",
    "            if self.epsilon > 1.0:\n",
    "                logger.warning(\n",
    "                    f\"Current epsilon value ({self.epsilon}) may be too high for a dataset of this size. \"\n",
    "                    \"For smaller datasets, consider using much smaller values (starting from 0.1) \"\n",
    "                    \"to ensure proper subset selection.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0602d",
   "metadata": {},
   "source": [
    "### EncoderConfig Class\n",
    "\n",
    "Configures the embedding encoder. Separates encoder settings from other parameters for modularity.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `instruction`: Prompt prefix for the encoder to guide embedding generation\n",
    "- `encoder_type`: Type of encoder to use (e.g., \"arctic\")\n",
    "- `encoder_model`: Specific model identifier (e.g., \"Snowflake/snowflake-arctic-embed-l-v2.0\")\n",
    "- `testing_mode`: If True, enables development features (CPU fallback, model auto-download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    \"\"\"Encoder-specific configuration parameters.\"\"\"\n",
    "    instruction: str = field(\n",
    "        default=\"Generate embeddings that capture the core meaning of user-assistant conversations, ensuring the embeddings can be clustered based on semantic similarity for subset selection.\",\n",
    "        metadata={\"advanced\": True},\n",
    "    )\n",
    "    encoder_type: str = field(default=\"arctic\", metadata={\"advanced\": True})\n",
    "    encoder_model: str = field(\n",
    "        default=\"Snowflake/snowflake-arctic-embed-l-v2.0\", metadata={\"advanced\": True}\n",
    "    )\n",
    "    testing_mode: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e96ce",
   "metadata": {},
   "source": [
    "### TemplateConfig Class\n",
    "\n",
    "Manages text formatting templates to enable flexible formatting for different data structures.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `template_name`: Active template to use (e.g., \"conversation\")\n",
    "- `templates`: Dictionary of available templates with Jinja2 syntax\n",
    "  - `default`: Simple text passthrough\n",
    "  - `conversation`: Formats multi-turn dialogues (user/assistant)\n",
    "  - `qa`: Question-answer format\n",
    "\n",
    "**Usage**: Templates convert structured data (dicts/lists) into plain text for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db28618",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TemplateConfig:\n",
    "    \"\"\"Template-related configuration parameters.\"\"\"\n",
    "    template_name: str = field(default=\"conversation\", metadata={\"advanced\": True})\n",
    "    templates: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"default\": \"{{ text }}\",\n",
    "            \"conversation\": \"{% for msg in messages if msg.role != 'system' %}{{ msg.role }}: {{ msg.content }}\\n{% endfor %}\",\n",
    "            \"qa\": \"Question: {{ question }}\\nAnswer: {{ answer }}\",\n",
    "        },\n",
    "        metadata={\"advanced\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51acd8c",
   "metadata": {},
   "source": [
    "### SystemConfig Class\n",
    "\n",
    "Manages system-level configuration for handling resources and error recovery.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `num_gpus`: Auto-detects available GPUs (set automatically in `__post_init__`)\n",
    "- `seed`: Random seed for reproducibility (default: 42)\n",
    "- `max_retries`: Number of retry attempts for failed operations (default: 3)\n",
    "- `retry_delay`: Seconds to wait between retries (default: 30)\n",
    "- `testing_mode`: Enables testing features (CPU fallback, reduced validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c409d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfig:\n",
    "    \"\"\"System-related configuration parameters.\"\"\"\n",
    "    num_gpus: int = field(init=False)\n",
    "    seed: int = field(default=42, metadata={\"advanced\": True})\n",
    "    max_retries: int = field(default=3, metadata={\"advanced\": True})\n",
    "    retry_delay: int = field(default=30, metadata={\"advanced\": True})\n",
    "    testing_mode: bool = field(default=False, metadata={\"advanced\": True})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize num_gpus after other fields are set.\"\"\"\n",
    "        self.num_gpus = get_default_num_gpus(testing_mode=self.testing_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c824d",
   "metadata": {},
   "source": [
    "### ProcessingConfig Class\n",
    "\n",
    "Main configuration class that combines all other configurations. Provides a single point of configuration with comprehensive validation.\n",
    "\n",
    "**Required Parameters:**\n",
    "- `input_files`: List of input file paths to process (JSONL format)\n",
    "- `subset_sizes`: List of target subset sizes\n",
    "  - Use **floats** (0-1) for percentages: `[0.1, 0.05]` = 10% and 5%\n",
    "  - Use **integers** for absolute counts: `[1000, 500]` = 1000 and 500 samples\n",
    "\n",
    "**Configuration Groups:**\n",
    "- `basic`: Basic processing parameters (output dir, batch size, epsilon)\n",
    "- `encoder`: Encoder-specific parameters (model, instruction, testing mode)\n",
    "- `template`: Template-related parameters (template name, Jinja2 templates)\n",
    "- `system`: System-related parameters (GPUs, seed, retries)\n",
    "\n",
    "**Validation**: Automatically validates subset sizes and parameter ranges in `__post_init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"\n",
    "    Configuration for subset selection with basic and advanced parameters.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Required parameters\n",
    "    input_files: List[str]\n",
    "    subset_sizes: List[Union[int, float]]\n",
    "\n",
    "    # Configuration groups\n",
    "    basic: BasicConfig = field(default_factory=BasicConfig)\n",
    "    encoder: EncoderConfig = field(default_factory=EncoderConfig)\n",
    "    template: TemplateConfig = field(default_factory=TemplateConfig)\n",
    "    system: SystemConfig = field(default_factory=SystemConfig)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        if not isinstance(self.subset_sizes, list):\n",
    "            raise ValueError(\"subset_sizes must be a list\")\n",
    "\n",
    "        for size in self.subset_sizes:\n",
    "            if not isinstance(size, (int, float)):\n",
    "                raise ValueError(\"subset_sizes must contain only integers or floats\")\n",
    "            if isinstance(size, float) and not 0 < size <= 100:\n",
    "                raise ValueError(\n",
    "                    \"Percentage values in subset_sizes must be between 0 and 100\"\n",
    "                )\n",
    "            if isinstance(size, int) and size <= 0:\n",
    "                raise ValueError(\"Absolute values in subset_sizes must be positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc14927",
   "metadata": {},
   "source": [
    "## Data Processor Class\n",
    "\n",
    "Main processing class that handles the complete data preparation pipeline with proper error handling.\n",
    "\n",
    "**Core Responsibilities:**\n",
    "- **Data Loading**: Loads and optionally combines multiple datasets from files\n",
    "- **Text Formatting**: Applies Jinja2 templates to convert structured data to text\n",
    "- **Subset Calculations**: Converts percentage/absolute subset sizes to actual sample counts\n",
    "- **Configuration Management**: Maintains all configuration and processing state\n",
    "- **Device Management**: Sets up GPU/CPU devices and random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Enhanced data processor with support for combined files and multiple selection methods.\n",
    "    \n",
    "    This class handles the complete pipeline:\n",
    "    - Data loading and formatting\n",
    "    - Embedding generation (to be implemented in Notebook 2)\n",
    "    - Subset selection (to be implemented in Notebook 3)\n",
    "    - Result export\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with the given configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ProcessingConfig): The processing configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.env = Environment(loader=BaseLoader())\n",
    "        self.templates = {\n",
    "            k: self.env.from_string(v) for k, v in config.template.templates.items()\n",
    "        }\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Set random seeds\n",
    "        np.random.seed(config.system.seed)\n",
    "        torch.manual_seed(config.system.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(config.system.seed)\n",
    "\n",
    "    def format_text(self, example: Dict[str, Any], format_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Formats the text of an example using the specified template.\n",
    "\n",
    "        Args:\n",
    "            example (Dict[str, Any]): The data example to format.\n",
    "            format_type (str): The key of the template to use.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted text.\n",
    "        \"\"\"\n",
    "        template = self.templates.get(format_type)\n",
    "        if not template:\n",
    "            raise ValueError(f\"Unknown format type: {format_type}\")\n",
    "        return template.render(**example)\n",
    "\n",
    "    def load_and_combine_datasets(self, input_files: List[str]):\n",
    "        \"\"\"\n",
    "        Load and optionally combine multiple datasets.\n",
    "\n",
    "        Args:\n",
    "            input_files (List[str]): List of input file paths.\n",
    "\n",
    "        Returns:\n",
    "            Combined dataset or list of individual datasets.\n",
    "        \"\"\"\n",
    "        datasets = []\n",
    "\n",
    "        for input_file in input_files:\n",
    "            file_extension = input_file.split(\".\")[-1]\n",
    "            if file_extension == \"jsonl\":\n",
    "                file_extension = \"json\"\n",
    "            dataset = load_dataset(\n",
    "                file_extension, data_files=input_file, split=\"train\", cache_dir=None\n",
    "            )\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        if self.config.basic.combine_files:\n",
    "            logger.info(\"Combining datasets...\")\n",
    "            return concatenate_datasets(datasets)\n",
    "\n",
    "        if len(datasets) > 1:\n",
    "            raise ValueError(\n",
    "                \"Multiple datasets provided but combine_files is not enabled\"\n",
    "            )\n",
    "        return datasets[0]\n",
    "\n",
    "    def calculate_subset_size(self, total_samples: int, size_spec: Union[int, float]) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the actual subset size based on the specification.\n",
    "        \n",
    "        Args:\n",
    "            total_samples (int): Total number of samples in the dataset.\n",
    "            size_spec (Union[int, float]): Size specification (percentage if float, absolute if int).\n",
    "\n",
    "        Returns:\n",
    "            int: Actual number of samples to select.\n",
    "        \"\"\"\n",
    "        if isinstance(size_spec, float):\n",
    "            # Handle percentage (0.1 = 10%, 0.05 = 5%)\n",
    "            if size_spec <= 0 or size_spec > 1:\n",
    "                raise ValueError(\n",
    "                    \"Percentage values must be between 0(non-inclusive) and 1(inclusive)\"\n",
    "                )\n",
    "            return max(1, int(size_spec * total_samples))\n",
    "        # Treat as absolute number\n",
    "        return min(size_spec, total_samples)\n",
    "\n",
    "    def get_subset_name(self, size_spec: Union[int, float], actual_size: int) -> str:\n",
    "        \"\"\"\n",
    "        Generate appropriate subset name based on selection method.\n",
    "\n",
    "        Args:\n",
    "            size_spec (Union[int, float]): Original size specification.\n",
    "            actual_size (int): Actual number of samples selected.\n",
    "\n",
    "        Returns:\n",
    "            str: Descriptive name for the subset.\n",
    "        \"\"\"\n",
    "        if isinstance(size_spec, float):\n",
    "            # Use :g format to automatically remove trailing zeros\n",
    "            # 0.1 -> \"0.1\", 0.05 -> \"0.05\", 0.10 -> \"0.1\"\n",
    "            return f\"percent_{size_spec:g}\"\n",
    "        return f\"samples_{actual_size}\"\n",
    "\n",
    "    def get_dataset_name(self, input_file: str) -> str:\n",
    "        \"\"\"\n",
    "        Get a clean dataset name from the input file path.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): Input file path\n",
    "\n",
    "        Returns:\n",
    "            str: Clean dataset name\n",
    "        \"\"\"\n",
    "        base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "        clean_name = re.sub(r\"[^\\w\\-_]\", \"_\", base_name)\n",
    "        return clean_name\n",
    "\n",
    "print(\"✅ DataProcessor class defined successfully with all utility functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b01ba3",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Create a complete configuration for the subset selection pipeline.\n",
    "\n",
    "**Key Configuration Decisions:**\n",
    "\n",
    "1. **Input Files**: Update `input_files` path to your JSONL dataset\n",
    "   - Expected format: One JSON object per line with `messages` field\n",
    "   \n",
    "2. **Subset Sizes**: `[0.1, 0.05]` creates two subsets (10% and 5% of original data)\n",
    "   - Use floats (0-1) for percentages of the dataset\n",
    "   - Use integers for absolute sample counts (e.g., `[1000, 500]`)\n",
    "   \n",
    "3. **Batch Size**: 100,000 samples per batch balances memory usage and processing speed\n",
    "   - Reduce if encountering OOM errors\n",
    "   \n",
    "4. **Epsilon**: 160.0 optimized for large datasets (>100K samples)\n",
    "   - Controls the trade-off between quality and speed in facility location\n",
    "   - For smaller datasets (<100K), use values starting from 0.1\n",
    "   \n",
    "5. **Testing Mode**: Enabled for development (allows CPU, auto-downloads models)\n",
    "   - Disable for production use with pre-downloaded models and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ProcessingConfig(\n",
    "    input_files=[\"../../assets/subset-selection/combined_cut_50x.jsonl\"], # Update with your data path\n",
    "    subset_sizes=[0.1, 0.05],  # 10% and 5% subsets\n",
    "    basic=BasicConfig(\n",
    "        output_dir=\"../../assets/subset-selection/outputs\",  # Change to your desired directory (used by Notebooks 2 & 3)\n",
    "        batch_size=100000,\n",
    "        num_folds=25,\n",
    "        epsilon=160.0,\n",
    "        combine_files=False\n",
    "    ),\n",
    "    encoder=EncoderConfig(\n",
    "        encoder_type=\"arctic\",\n",
    "        encoder_model=\"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
    "        testing_mode=True  # Enable for notebook development\n",
    "    ),\n",
    "    template=TemplateConfig(\n",
    "        template_name=\"conversation\",\n",
    "        templates={\n",
    "            \"default\": \"{{ text }}\",\n",
    "            \"conversation\": \"{% for msg in messages if msg.role != 'system' %}{{ msg.role }}: {{ msg.content }}\\n{% endfor %}\",\n",
    "            \"qa\": \"Question: {{ question }}\\nAnswer: {{ answer }}\",\n",
    "        }\n",
    "    ),\n",
    "    system=SystemConfig(\n",
    "        seed=42,\n",
    "        testing_mode=True,\n",
    "        max_retries=3,\n",
    "        retry_delay=30\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"===== Configuration created successfully! =====\")\n",
    "print(f\"Input files: {config.input_files}\")\n",
    "print(f\"Subset sizes: {config.subset_sizes}\")\n",
    "print(f\"Output directory: {config.basic.output_dir}\")\n",
    "print(f\"Encoder type: {config.encoder.encoder_type}\")\n",
    "print(f\"Template name: {config.template.template_name}\")\n",
    "print(f\"Number of GPUs: {config.system.num_gpus}\")\n",
    "print(f\"Testing mode: {config.system.testing_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701330f",
   "metadata": {},
   "source": [
    "## Data Loading and Validate input data\n",
    "- Checks if data file exists\n",
    "- Loads dataset using HuggingFace datasets\n",
    "- Shows dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📁 Data Loading\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if data file exists\n",
    "data_file = config.input_files[0]\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"❌ Data file not found: {data_file}\")\n",
    "    print(\"Please update the data_file path in the configuration above\")\n",
    "    print(\"Example data file structure:\")\n",
    "    print(\"\"\"\n",
    "    [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you!\"}\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(f\"✅ Found data file: {data_file}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    try:\n",
    "        dataset = load_dataset(\"json\", data_files=data_file, split=\"train\", cache_dir=None)\n",
    "        print(f\"✅ Dataset loaded successfully!\")\n",
    "        print(f\"📊 Dataset size: {len(dataset):,} samples\")\n",
    "        \n",
    "        # Show file size\n",
    "        file_size = os.path.getsize(data_file) / 1024**2  # MB\n",
    "        print(f\"📁 File size: {file_size:.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading dataset: {e}\")\n",
    "        dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e32d1b",
   "metadata": {},
   "source": [
    "## Data Inspection - show data structure\n",
    "It analyzes loaded data structure and quality\n",
    "- Displays sample data\n",
    "- Analyzes column structure\n",
    "- Counts messages and roles\n",
    "- validates epsilon for dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is not None:\n",
    "    print(\"🔍 Data Inspection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"📋 Sample data:\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        sample = dataset[i]\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"  {key}: {value[:100]}...\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Analyze data structure\n",
    "    print(f\"\\n📊 Data structure analysis:\")\n",
    "    print(f\"   Number of samples: {len(dataset):,}\")\n",
    "    \n",
    "    # Check column names\n",
    "    if hasattr(dataset, 'column_names'):\n",
    "        print(f\"   Column names: {dataset.column_names}\")\n",
    "    \n",
    "    # Analyze message structure if it exists\n",
    "    if 'messages' in dataset.column_names:\n",
    "        message_lengths = []\n",
    "        role_counts = {'user': 0, 'assistant': 0, 'system': 0}\n",
    "        \n",
    "        for sample in dataset.select(range(min(1000, len(dataset)))):\n",
    "            if 'messages' in sample:\n",
    "                message_lengths.append(len(sample['messages']))\n",
    "                for msg in sample['messages']:\n",
    "                    if 'role' in msg:\n",
    "                        role_counts[msg['role']] += 1\n",
    "        \n",
    "        print(f\"Average messages per conversation: {np.mean(message_lengths):.1f}\")\n",
    "        print(f\"Role distribution: {role_counts}\")\n",
    "    \n",
    "    # Validate epsilon for dataset size\n",
    "    config.basic.validate_epsilon_for_dataset_size(len(dataset))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No dataset loaded. Please fix the data loading issue above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_next_steps",
   "metadata": {},
   "source": [
    "### 🎯 Next Steps\n",
    "\n",
    "**Proceed to Notebook 2: Embedding Generation**\n",
    "\n",
    "1. Open `embedding_generation.ipynb`\n",
    "2. Run all cells sequentially\n",
    "3. The notebook will:\n",
    "   - Import all objects from this notebook using `%run`\n",
    "   - Add `generate_embeddings()` method to `DataProcessor`\n",
    "   - Generate embeddings using Arctic encoder on GPU(s)\n",
    "   - Save embeddings to `{output_dir}/embeddings/embeddings.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b41dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
