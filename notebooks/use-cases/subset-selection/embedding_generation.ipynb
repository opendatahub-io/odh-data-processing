{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b190f0",
   "metadata": {},
   "source": [
    "# Subset Selection Notebook 2: Embedding Generation\n",
    "\n",
    "## Overview\n",
    "This notebook is the second step in the **Subset Selection Pipeline**. It focuses on generating high-quality embeddings from the formatted text data prepared in Notebook 1.\n",
    "\n",
    "## Purpose in Subset Selection\n",
    "Embeddings are vector representations of text that capture semantic meaning. In subset selection, we need embeddings to:\n",
    "1. **Measure Similarity**: Calculate how similar different data samples are to each other\n",
    "2. **Enable Facility Location**: The subset selection algorithm uses embeddings to identify diverse, representative samples\n",
    "3. **Preserve Semantic Information**: Ensure selected subsets maintain the semantic diversity of the original dataset\n",
    "\n",
    "## Output\n",
    "- **embeddings.h5**: Merged embedding file containing vector representations of all samples\n",
    "- Used in Notebook 3 for subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f415d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Notebook 1 using %run magic command\n",
    "# This executes the entire Notebook 1 in the current namespace\n",
    "%run \"data_preparation_and_config.ipynb\"\n",
    "\n",
    "# Additional imports needed for embedding generation\n",
    "from multiprocessing import Pool\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "# Third Party Imports (not in Notebook 1)\n",
    "import h5py\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from typing import TypedDict, Union, List, Dict, Optional, Any\n",
    "\n",
    "# Set up logger for this notebook\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Successfully imported from Notebook 1!\")\n",
    "print(f\"   ‚Ä¢ config: {type(config).__name__ if 'config' in locals() else 'Not defined'}\")\n",
    "print(f\"   ‚Ä¢ dataset: {len(dataset) if 'dataset' in locals() and dataset else 'None'} samples\")\n",
    "\n",
    "print(\"\\nüì¶ Notebook 2 Components Loading...\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922c214",
   "metadata": {},
   "source": [
    "### Arctic Encoder Model Configuration\n",
    "Defines the configuration structure and settings for the Snowflake Arctic embedding model.\n",
    "**Classes Defined:**\n",
    "1. **`ModelConfig`** (TypedDict): Schema for model-specific settings\n",
    "2. **`ArcticEncoderConfig`** (Dataclass): Internal configuration for the encoder instance\n",
    "3. **`MODEL_CONFIGS`** (Dict): Pre-configured settings for supported models\n",
    "\n",
    "**Arctic Model Settings:**\n",
    "- **Pooling Method**: CLS token (first token of sequence)\n",
    "- **Normalization**: L2-normalized embeddings for cosine similarity\n",
    "- **Max Length**: 4096 tokens (handles long documents)\n",
    "- **Default Instruction**: \"Retrieve relevant passages:\"\n",
    "- **Batch Size**: 24 samples per batch\n",
    "  - Optimized for GPU memory on typical setups (8-24GB VRAM)\n",
    "  - With 4096 max length and L-v2.0 model, fits comfortably in memory\n",
    "  - Reduce to 12-16 if encountering OOM errors\n",
    "  - Can increase to 32-48 on high-memory GPUs (A100, H100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc44c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration \n",
    "class ModelConfig(TypedDict):\n",
    "    pooling_method: str\n",
    "    normalize_embeddings: bool\n",
    "    max_length: int\n",
    "    default_instruction: str\n",
    "    batch_size: int\n",
    "\n",
    "@dataclass\n",
    "class ArcticEncoderConfig:\n",
    "    \"\"\"Encoder configuration for Arctic model.\"\"\"\n",
    "    model_name: str\n",
    "    model_config: ModelConfig\n",
    "    device: torch.device\n",
    "    num_gpus: int\n",
    "    batch_size: int\n",
    "    use_default_instruction: bool\n",
    "    use_fp16: bool\n",
    "    testing_mode: bool = False\n",
    "\n",
    "# Model configurations \n",
    "MODEL_CONFIGS: Dict[str, ModelConfig] = {\n",
    "    \"Snowflake/snowflake-arctic-embed-l-v2.0\": {\n",
    "        \"pooling_method\": \"cls\",\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"max_length\": 4096,\n",
    "        \"default_instruction\": \"Retrieve relevant passages:\",\n",
    "        \"batch_size\": 24,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b412ba",
   "metadata": {},
   "source": [
    "### ArcticEmbedEncoder Implementation\n",
    "\n",
    "Implements the complete Arctic embedding encoder with the following capabilities:\n",
    "\n",
    "**Encoder Registry:**\n",
    "- `ENCODER_REGISTRY`: Maps encoder types to classes\n",
    "- `get_encoder_class()`: Factory function to get encoder by type\n",
    "\n",
    "**Design**: Each encoder instance runs on a single GPU for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcticEmbedEncoder:\n",
    "    \"\"\"\n",
    "    Arctic embedding encoder for generating high-quality text embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
    "        device: Optional[torch.device] = None,\n",
    "        use_fp16: bool = False,\n",
    "        use_default_instruction: bool = True,\n",
    "        testing_mode: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes encoder with specified model\n",
    "            Sets up GPU device\n",
    "            Creates configuration\n",
    "            Calls _initialize_model()\n",
    "        \"\"\"\n",
    "        if model_name not in MODEL_CONFIGS:\n",
    "            raise ValueError(\n",
    "                f\"Model {model_name} not supported. Supported models: {list(MODEL_CONFIGS.keys())}\"\n",
    "            )\n",
    "\n",
    "        # Use the provided device or default to CUDA\n",
    "        self.device = device or torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Get device ID for logging\n",
    "        self.device_id = self.device.index if hasattr(self.device, \"index\") else 0\n",
    "\n",
    "        # Configuration\n",
    "        self.cfg = ArcticEncoderConfig(\n",
    "            model_name=model_name,\n",
    "            model_config=MODEL_CONFIGS[model_name],\n",
    "            device=self.device,\n",
    "            num_gpus=1,  # Only use 1 GPU per encoder instance\n",
    "            batch_size=MODEL_CONFIGS[model_name][\"batch_size\"],\n",
    "            use_default_instruction=use_default_instruction,\n",
    "            use_fp16=use_fp16,\n",
    "            testing_mode=testing_mode,\n",
    "        )\n",
    "\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> None:\n",
    "        \"\"\"Loads tokenizer and model from local cache or HuggingFace\n",
    "            In testing mode: downloads from HuggingFace\n",
    "            In production: requires pre-downloaded model\n",
    "            Moves model to GPU and sets to evaluation mode\n",
    "        \"\"\"\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        model_path = os.path.join(\n",
    "            home_dir, \".cache\", \"instructlab\", \"models\", self.cfg.model_name\n",
    "        )\n",
    "\n",
    "        # In testing mode, allow direct download from HuggingFace\n",
    "        if hasattr(self.cfg, \"testing_mode\") and self.cfg.testing_mode:\n",
    "            logger.warning(\n",
    "                f\"Model not found locally at {model_path}. \"\n",
    "                \"Testing mode enabled - downloading from HuggingFace...\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.model_name)\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.cfg.model_name,\n",
    "                add_pooling_layer=False,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "        else:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise ValueError(\n",
    "                    f\"Model not found in available models: {self.cfg.model_name}\\n\"\n",
    "                    \"Please run `ilab model download` and download the necessary model\"\n",
    "                )\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                model_path,\n",
    "                add_pooling_layer=False,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "\n",
    "        if self.cfg.use_fp16:\n",
    "            self.model = self.model.half()\n",
    "\n",
    "        self.model = self.model.to(self.cfg.device)\n",
    "        logger.info(f\"Model loaded on device: {self.cfg.device}\")\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "    def _prepare_inputs(\n",
    "        self, texts: Union[str, List[str]], instruction: str = \"\"\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Adds instruction prefix to texts\n",
    "            Ensures instruction is always present\n",
    "            Formats inputs for the model\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        # Ensure we always have an instruction\n",
    "        if not instruction and not self.cfg.use_default_instruction:\n",
    "            raise ValueError(\n",
    "                \"An instruction must be provided when use_default_instruction is False. \"\n",
    "                \"Either provide an instruction or set use_default_instruction to True.\"\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            not instruction\n",
    "            and self.cfg.use_default_instruction\n",
    "            and self.cfg.model_config[\"default_instruction\"]\n",
    "        ):\n",
    "            instruction = str(self.cfg.model_config[\"default_instruction\"])\n",
    "\n",
    "        if not instruction:  # catch if default_instruction is empty\n",
    "            raise ValueError(\n",
    "                \"No instruction available. Either provide an instruction or ensure \"\n",
    "                \"the model config has a valid default_instruction.\"\n",
    "            )\n",
    "\n",
    "        texts = [f\"{instruction}: {text}\" for text in texts]\n",
    "        return texts\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(\n",
    "        self,\n",
    "        inputs: Union[str, List[str]],\n",
    "        instruction: str = \"\",\n",
    "        return_tensors: bool = True,\n",
    "        show_progress: bool = True,\n",
    "    ) -> Union[torch.Tensor, np.ndarray]:\n",
    "        \"\"\"Main method to generate embeddings\n",
    "            Tokenizes input texts\n",
    "            Processes in batches\n",
    "            Applies CLS pooling and L2 normalization\n",
    "            Returns PyTorch tensors or numpy arrays\n",
    "        \"\"\"\n",
    "        input_was_string = isinstance(inputs, str)\n",
    "        inputs = self._prepare_inputs(inputs, instruction)\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.cfg.model_config[\"max_length\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.cfg.device)\n",
    "\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(\n",
    "            range(0, len(inputs), self.cfg.batch_size),\n",
    "            disable=not show_progress or len(inputs) < 256,\n",
    "            desc=f\"Encoding on {self.device}\",\n",
    "        ):\n",
    "            batch = {k: v[i : i + self.cfg.batch_size] for k, v in encodings.items()}\n",
    "            outputs = self.model(**batch)\n",
    "            # Take the first token embedding (CLS) and normalize it\n",
    "            embeddings = F.normalize(outputs.last_hidden_state[:, 0], p=2, dim=1)\n",
    "            embeddings_list.append(embeddings.cpu())\n",
    "\n",
    "        embeddings = torch.cat(embeddings_list, dim=0)\n",
    "        if input_was_string:\n",
    "            embeddings = embeddings[0]\n",
    "\n",
    "        return embeddings if return_tensors else embeddings.numpy()\n",
    "\n",
    "\n",
    "# Encoder Registry\n",
    "ENCODER_REGISTRY = {\n",
    "    \"arctic\": ArcticEmbedEncoder,\n",
    "}\n",
    "\n",
    "def get_encoder_class(encoder_type: str):\n",
    "    \"\"\"Get the encoder class based on the encoder type.\"\"\"\n",
    "    try:\n",
    "        if encoder_type not in ENCODER_REGISTRY:\n",
    "            supported_encoders = list(ENCODER_REGISTRY.keys())\n",
    "            raise ValueError(\n",
    "                f\"Unsupported encoder type: '{encoder_type}'. \"\n",
    "                f\"Supported types are: {supported_encoders}\"\n",
    "            )\n",
    "        return ENCODER_REGISTRY[encoder_type]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error getting encoder class: {str(e)}\") from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be02656",
   "metadata": {},
   "source": [
    "### Pairwise Similarity Computation\n",
    "\n",
    "Defines the `compute_pairwise_dense()` function for calculating similarities between embeddings.\n",
    "\n",
    "**What This Function Does:**\n",
    "- Computes pairwise metrics (cosine, euclidean, RBF) between two sets of vectors\n",
    "- Processes in batches to avoid GPU memory overflow\n",
    "- Supports multiple similarity metrics\n",
    "- Applies optional scaling (min-max or additive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9556da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_dense(\n",
    "    tensor1: torch.Tensor,\n",
    "    tensor2: Optional[torch.Tensor] = None,\n",
    "    batch_size: int = 10000,\n",
    "    metric: str = \"cosine\",\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    scaling: Optional[str] = None,\n",
    "    kw: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pairwise metric in batches between two sets of vectors.\n",
    "    This function is needed for similarity computation in subset selection (Notebook 3).\n",
    "    - `tensor1`, `tensor2`: Input embedding tensors\n",
    "    - `batch_size`: Size of processing batches (default: 10K)\n",
    "    - `metric`: Similarity metric (\"cosine\", \"euclidean\", \"rbf\", \"dot\")\n",
    "    - `device`: GPU device to use\n",
    "    - `scaling`: Optional scaling (\"min-max\", \"additive\", None)\n",
    "    \"\"\"\n",
    "    assert batch_size > 0, \"Batch size must be positive.\"\n",
    "\n",
    "    if not device:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if tensor2 is None:\n",
    "        tensor2 = tensor1\n",
    "\n",
    "    tensor1, tensor2 = tensor1.to(device), tensor2.to(device)\n",
    "    n_samples1, n_samples2 = tensor1.size(0), tensor2.size(0)\n",
    "    results = torch.zeros(n_samples1, n_samples2, device=\"cpu\")\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        tensor1, tensor2 = (\n",
    "            F.normalize(tensor1, p=2, dim=1),\n",
    "            F.normalize(tensor2, p=2, dim=1),\n",
    "        )\n",
    "\n",
    "    def calculate_metric(a: torch.Tensor, b: torch.Tensor, metric: str, kw: float) -> torch.Tensor:\n",
    "        if metric in [\"cosine\", \"dot\"]:\n",
    "            return torch.mm(a, b.T)\n",
    "        if metric == \"euclidean\":\n",
    "            distances = torch.cdist(a, b, p=2)\n",
    "            similarities = 1 / (1 + distances**2)\n",
    "            return similarities\n",
    "        if metric == \"rbf\":\n",
    "            distance = torch.cdist(a, b)\n",
    "            squared_distance = distance**2\n",
    "            avg_dist = torch.mean(squared_distance)\n",
    "            torch.div(squared_distance, kw * avg_dist, out=squared_distance)\n",
    "            torch.exp(-squared_distance, out=squared_distance)\n",
    "            return squared_distance\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "    for i in range(0, n_samples1, batch_size):\n",
    "        end_i = min(i + batch_size, n_samples1)\n",
    "        rows = tensor1[i:end_i]\n",
    "\n",
    "        for j in range(0, n_samples2, batch_size):\n",
    "            end_j = min(j + batch_size, n_samples2)\n",
    "            cols = tensor2[j:end_j]\n",
    "            batch_results = calculate_metric(rows, cols, metric, kw).cpu()\n",
    "            results[i:end_i, j:end_j] = batch_results\n",
    "\n",
    "    if scaling == \"min-max\":\n",
    "        min_val, max_val = results.min(), results.max()\n",
    "        if max_val != min_val:\n",
    "            results = (results - min_val) / (max_val - min_val)\n",
    "    elif scaling == \"additive\":\n",
    "        results = (results + 1) / 2\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d15888",
   "metadata": {},
   "source": [
    "### Multi-GPU Embedding Generation Functions\n",
    "Implements parallel embedding generation across multiple GPUs.\n",
    "\n",
    "**Workflow:**\n",
    "Dataset ‚Üí Split into N shards ‚Üí Process on N GPUs in parallel ‚Üí Merge results\n",
    "n-samples ‚Üí GPU 0: n-samples ‚Üí embeddings.h5\n",
    "(or split across multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_dataset_shard(args):\n",
    "    \"\"\"\n",
    "    Processes one shard of data on a specific GPU\n",
    "   - Creates encoder instance on assigned GPU\n",
    "   - Applies templates to format text\n",
    "   - Generates embeddings in batches\n",
    "   - Saves embeddings to shard-specific HDF5 file\n",
    "   - Includes progress bar for monitoring\n",
    "    \"\"\"\n",
    "    (\n",
    "        gpu_id,\n",
    "        dataset_shard,\n",
    "        output_dir,\n",
    "        encoder_type,\n",
    "        encoder_model,\n",
    "        instruction,\n",
    "        template_name,\n",
    "        templates,\n",
    "        batch_size,\n",
    "        testing_mode,\n",
    "    ) = args\n",
    "\n",
    "    try:\n",
    "        # Set the GPU for this process\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "            device = f\"cuda:{gpu_id}\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            \n",
    "        logger.info(f\"GPU {gpu_id} started processing {len(dataset_shard)} samples\")\n",
    "\n",
    "        encoder_cls = get_encoder_class(encoder_type)\n",
    "        encoder = encoder_cls(\n",
    "            model_name=encoder_model,\n",
    "            device=torch.device(device),\n",
    "            testing_mode=testing_mode,\n",
    "        )\n",
    "\n",
    "        # Set up Jinja environment for templating\n",
    "        env = Environment(loader=BaseLoader())\n",
    "        templates_dict = {k: env.from_string(v) for k, v in templates.items()}\n",
    "\n",
    "        # Create shard-specific output directory\n",
    "        shard_dir = os.path.join(output_dir, f\"shard_{gpu_id}\")\n",
    "        os.makedirs(shard_dir, exist_ok=True)\n",
    "\n",
    "        # Process batches\n",
    "        all_embeddings = []\n",
    "        batch_texts = []\n",
    "\n",
    "        # Create progress bar\n",
    "        progress_bar = tqdm(\n",
    "            desc=f\"GPU {gpu_id} generating embeddings\",\n",
    "            total=len(dataset_shard),\n",
    "            unit=\" samples\",\n",
    "            position=gpu_id,\n",
    "            leave=True,\n",
    "        )\n",
    "\n",
    "        # Process each example in the shard\n",
    "        for example in dataset_shard:\n",
    "            # Format the text using the template\n",
    "            template = templates_dict.get(template_name)\n",
    "            if not template:\n",
    "                raise ValueError(f\"Unknown format type: {template_name}\")\n",
    "\n",
    "            text = template.render(**example)\n",
    "            batch_texts.append(text)\n",
    "\n",
    "            # Process when batch is full or at the end\n",
    "            if len(batch_texts) == batch_size or example == dataset_shard[-1]:\n",
    "                # Generate embeddings for this batch\n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = (\n",
    "                        encoder.encode(\n",
    "                            inputs=batch_texts,\n",
    "                            instruction=instruction,\n",
    "                            return_tensors=False,  # Return numpy for easier handling\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "                progress_bar.update(len(batch_texts))\n",
    "                batch_texts = []\n",
    "\n",
    "                # Clean up GPU memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        # Concatenate all batches\n",
    "        if not all_embeddings:\n",
    "            logger.warning(f\"No embeddings generated for shard on GPU {gpu_id}\")\n",
    "            return None\n",
    "\n",
    "        embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "        # Save embeddings to file\n",
    "        shard_file = os.path.join(shard_dir, f\"embeddings_shard_{gpu_id}.h5\")\n",
    "        with h5py.File(shard_file, \"w\") as h5f:\n",
    "            h5f.create_dataset(\"embeddings\", data=embeddings, dtype=\"float32\")\n",
    "\n",
    "        logger.info(f\"GPU {gpu_id} completed processing. Saved to {shard_file}\")\n",
    "        return shard_file\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing shard on GPU {gpu_id}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def _merge_shard_files(shard_files, merged_file):\n",
    "    \"\"\"\n",
    "    Combines all shard files into single embeddings file\n",
    "    - Preserves embedding dimension and data type\n",
    "    - Removes shard files after merging\n",
    "    - Creates final `embeddings.h5` file\n",
    "    \"\"\"\n",
    "    logger.info(f\"Merging {len(shard_files)} shard files into {merged_file}\")\n",
    "\n",
    "    # Get the shape and type of embeddings from the first shard\n",
    "    with h5py.File(shard_files[0], \"r\") as f:\n",
    "        first_embeddings = f[\"embeddings\"]\n",
    "        embedding_dim = first_embeddings.shape[1]\n",
    "        dtype = first_embeddings.dtype\n",
    "\n",
    "    # Count total samples across all shards\n",
    "    total_samples = 0\n",
    "    for shard_file in shard_files:\n",
    "        with h5py.File(shard_file, \"r\") as f:\n",
    "            total_samples += f[\"embeddings\"].shape[0]\n",
    "\n",
    "    # Create the merged file\n",
    "    with h5py.File(merged_file, \"w\") as merged_f:\n",
    "        merged_dataset = merged_f.create_dataset(\n",
    "            \"embeddings\", shape=(total_samples, embedding_dim), dtype=dtype\n",
    "        )\n",
    "\n",
    "        # Copy embeddings from each shard\n",
    "        start_idx = 0\n",
    "        for shard_file in shard_files:\n",
    "            with h5py.File(shard_file, \"r\") as shard_f:\n",
    "                embeddings = shard_f[\"embeddings\"][:]\n",
    "                end_idx = start_idx + embeddings.shape[0]\n",
    "                merged_dataset[start_idx:end_idx] = embeddings\n",
    "                start_idx = end_idx\n",
    "\n",
    "            # Remove shard file after merging\n",
    "            os.remove(shard_file)\n",
    "            # Remove shard directory if empty\n",
    "            shard_dir = os.path.dirname(shard_file)\n",
    "            if not os.listdir(shard_dir):\n",
    "                os.rmdir(shard_dir)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Successfully merged embeddings from {len(shard_files)} GPUs with {total_samples} total samples\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac23ad3",
   "metadata": {},
   "source": [
    "### DataProcessor with Embedding Generation\n",
    "Adds the `generate_embeddings()` method to the DataProcessor class from Notebook 1.\n",
    "\n",
    "**Processing Modes:**\n",
    "- **Testing Mode / Single GPU**: Serial processing (notebook-friendly)\n",
    "- **Production / Multi-GPU**: Parallel processing with multiprocessing.Pool\n",
    "**Decorated with `@retry_on_exception`:**\n",
    "- Automatically retries on GPU OOM errors\n",
    "- Cleans up memory between retries\n",
    "- Uses retry settings from SystemConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry_on_exception\n",
    "def generate_embeddings(self, dataset, output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates embeddings for the dataset and saves them to the output directory,\n",
    "    using multiple GPUs in parallel.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to process.\n",
    "        output_dir (str): The directory where embeddings will be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the merged embeddings file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    merged_path = os.path.join(output_dir, \"embeddings.h5\")\n",
    "\n",
    "    # If embeddings already exist, return early\n",
    "    if os.path.exists(merged_path):\n",
    "        logger.info(f\"Embeddings file already exists in {output_dir}, skipping\")\n",
    "        return merged_path\n",
    "\n",
    "    # Get number of GPUs to use\n",
    "    num_gpus = min(\n",
    "        self.config.system.num_gpus,\n",
    "        torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    )\n",
    "    logger.info(f\"Using {num_gpus} GPUs for embedding generation\")\n",
    "\n",
    "    # Create dataset shards - one per GPU\n",
    "    total_samples = len(dataset)\n",
    "    per_gpu_samples = (total_samples + num_gpus - 1) // num_gpus  # Ceiling division\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        # Calculate start and end indices for this shard\n",
    "        start_idx = gpu_id * per_gpu_samples\n",
    "        end_idx = min(start_idx + per_gpu_samples, total_samples)\n",
    "\n",
    "        if start_idx >= total_samples:\n",
    "            continue  # Skip if this GPU has no data to process\n",
    "\n",
    "        # Create arguments for this GPU\n",
    "        args_list.append(\n",
    "            (\n",
    "                gpu_id,\n",
    "                dataset.select(range(start_idx, end_idx)),\n",
    "                output_dir,\n",
    "                self.config.encoder.encoder_type,\n",
    "                self.config.encoder.encoder_model,\n",
    "                self.config.encoder.instruction,\n",
    "                self.config.template.template_name,\n",
    "                self.config.template.templates,\n",
    "                self.config.basic.batch_size,\n",
    "                self.config.encoder.testing_mode,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Process dataset shards\n",
    "    # Use serial processing in testing mode (notebook-friendly)\n",
    "    # Use parallel processing in production mode\n",
    "    if self.config.encoder.testing_mode or num_gpus == 1:\n",
    "        logger.info(\"Processing shards serially (testing mode or single GPU)\")\n",
    "        shard_files = []\n",
    "        for args in args_list:\n",
    "            result = _process_dataset_shard(args)\n",
    "            shard_files.append(result)\n",
    "    else:\n",
    "        logger.info(f\"Processing shards in parallel with {num_gpus} workers\")\n",
    "        with Pool(processes=num_gpus) as pool:\n",
    "            shard_files = pool.map(_process_dataset_shard, args_list)\n",
    "\n",
    "    # Filter out None values (failed shards)\n",
    "    shard_files = [f for f in shard_files if f is not None]\n",
    "\n",
    "    if not shard_files:\n",
    "        raise ValueError(\"No embeddings were generated from any GPU\")\n",
    "\n",
    "    # Merge all shard files\n",
    "    _merge_shard_files(shard_files, merged_path)\n",
    "\n",
    "    return merged_path\n",
    "\n",
    "\n",
    "# Add the method to DataProcessor class\n",
    "DataProcessor.generate_embeddings = generate_embeddings\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All Notebook 2 Components Loaded Successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ Components available:\")\n",
    "print(\"   ‚Ä¢ Arctic Encoder classes and registry\")\n",
    "print(\"   ‚Ä¢ Pairwise similarity computation function\")\n",
    "print(\"   ‚Ä¢ Multi-GPU processing functions\")\n",
    "print(\"   ‚Ä¢ generate_embeddings() method added to DataProcessor\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüéØ Ready to generate embeddings!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573e915",
   "metadata": {},
   "source": [
    "### üöÄ Execute Embedding Generation\n",
    "\n",
    "Run this cell to process your dataset and generate embeddings.\n",
    "\n",
    "**What happens:**\n",
    "1. Validates dataset and configuration\n",
    "2. Splits data across available GPUs\n",
    "3. Generates embeddings using Arctic encoder\n",
    "4. Merges results into single HDF5 file\n",
    "5. Reports timing and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c706662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Multi-GPU Embedding Generation\n",
    "\n",
    "if 'dataset' in locals() and dataset is not None:\n",
    "    print(\"üéØ Multi-GPU Embedding Generation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create DataProcessor instance if it doesn't exist\n",
    "    if 'processor' not in locals():\n",
    "        processor = DataProcessor(config)\n",
    "        print(\"‚úÖ Created DataProcessor instance\")\n",
    "    \n",
    "    # Set up output directory\n",
    "    output_dir = os.path.join(config.basic.output_dir, \"embeddings\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset size: {len(dataset):,} samples\")\n",
    "    print(f\"üíæ Output directory: {output_dir}\")\n",
    "    print(f\"üéØ Number of GPUs: {config.system.num_gpus}\")\n",
    "    print(f\"üì¶ Batch size: {config.basic.batch_size}\")\n",
    "    \n",
    "    # Generate embeddings using the extended DataProcessor\n",
    "    print(f\"\\nüöÄ Starting multi-GPU embedding generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        embeddings_file = processor.generate_embeddings(dataset, output_dir)\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Embedding generation completed!\")\n",
    "        print(f\"‚è±Ô∏è  Total time: {generation_time / 60:.2f} minutes\")\n",
    "        print(f\"üéØ Speed: {len(dataset) / generation_time:.2f} samples/sec\")\n",
    "        print(f\"üíæ Embeddings saved to: {embeddings_file}\")\n",
    "        \n",
    "        # Show file size\n",
    "        file_size = os.path.getsize(embeddings_file) / 1024**2\n",
    "        print(f\"üìÅ File size: {file_size:.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during embedding generation: {e}\")\n",
    "        embeddings_file = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate embeddings without dataset\")\n",
    "    print(\"Please ensure Notebook 1 has been run and dataset was loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "success_summary",
   "metadata": {},
   "source": [
    "### üéØ Next Steps\n",
    "\n",
    "**Option 1: Continue to Notebook 3 (Recommended)**\n",
    "1. Open `subset_selection.ipynb`\n",
    "2. Run the notebook sequentially\n",
    "3. Use the embeddings generated here for subset selection\n",
    "4. This is the FAST path - no redundant computation!\n",
    "\n",
    "**Option 2: Reuse These Embeddings Later**\n",
    "- The embeddings file is saved and can be loaded anytime\n",
    "- Path: `{config.basic.output_dir}/embeddings/embeddings.h5`\n",
    "- Use `h5py.File()` to load them in any notebook or script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c316f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
