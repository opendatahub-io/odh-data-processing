{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2569cf0f",
   "metadata": {},
   "source": [
    "# Subset Selection Notebook 3: Subset Selection & Results\n",
    "\n",
    "## Overview\n",
    "This notebook is the **final step** in the Subset Selection Pipeline. It uses the embeddings generated in Notebook 2 to select diverse, representative subsets using the submodular optimization algorithm.\n",
    "\n",
    "## Purpose in Subset Selection\n",
    "The Facility Location algorithm selects a subset of samples that:\n",
    "1. **Maximizes Diversity**: Selects samples that are representative of the entire dataset\n",
    "2. **Minimizes Redundancy**: Avoids selecting similar samples\n",
    "3. **Optimizes Coverage**: Ensures all semantic regions of the dataset are represented\n",
    "4. **Maintains Quality**: Preserves the distribution and characteristics of the original dataset\n",
    "\n",
    "## Output\n",
    "- **Subset JSONL files**: Selected subsets in JSONL format (e.g., `combined_cut_50x_percent_0.1_subset.jsonl`)\n",
    "- **Metadata files**: `.npz` files containing selected indices and gain scores for each subset\n",
    "- **Statistics**: Summary of subset sizes, selection time, and quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Notebook 2 (which already imports Notebook 1)\n",
    "%run \"embedding_generation.ipynb\"\n",
    "\n",
    "# Additional imports for subset selection\n",
    "import math\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import TypedDict, Union, List, Dict, Optional, Any\n",
    "# Import submodlib for Facility Location\n",
    "from submodlib import FacilityLocationFunction\n",
    "\n",
    "# Set up logger for this notebook\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Successfully imported from previous notebooks!\")\n",
    "print(f\"   • config: {type(config).__name__ if 'config' in locals() else 'Not defined'}\")\n",
    "print(f\"   • dataset: {len(dataset) if 'dataset' in locals() and dataset else 'None'} samples\")\n",
    "print(f\"   • processor: {type(processor).__name__ if 'processor' in locals() else 'Not defined'}\")\n",
    "print(f\"   • DataProcessor has generate_embeddings(): {hasattr(processor, 'generate_embeddings') if 'processor' in locals() else False}\")\n",
    "print(f\"\\n✅ Subset selection libraries loaded!\")\n",
    "print(f\"   • submodlib: FacilityLocationFunction available\")\n",
    "print(f\"   • All utility functions from previous notebooks available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0c0f2",
   "metadata": {},
   "source": [
    "### GPU-Based Fold Processing\n",
    "\n",
    "Implements the core subset selection algorithm using facility location.\n",
    "\n",
    "**Why Folds?**\n",
    "The dataset is split into multiple folds for three key reasons:\n",
    "1. **Memory Efficiency**: Processing smaller chunks prevents GPU out-of-memory errors\n",
    "2. **Parallelization**: Multiple GPUs can work on different folds simultaneously\n",
    "3. **Better Coverage**: Cross-validation-like approach ensures diverse sample selection\n",
    "\n",
    "**How It Works:**\n",
    "```\n",
    "Full Dataset (N samples)\n",
    "↓\n",
    "Split into K folds (e.g., 25 folds)\n",
    "↓\n",
    "Assign folds to GPUs\n",
    "↓\n",
    "For each fold:\n",
    "- Compute pairwise similarities\n",
    "- Run facility location algorithm\n",
    "- Select diverse samples with high gains\n",
    "↓\n",
    "Merge results from all folds\n",
    "↓\n",
    "Select top samples by gain scores\n",
    "```\n",
    "**Algorithm**: Uses LazierThanLazyGreedy optimizer for efficient submodular maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folds_with_gpu(args):\n",
    "    \"\"\"\n",
    "    Process folds on GPU or CPU with support for both percentage and absolute size specifications.\n",
    "    \"\"\"\n",
    "    (\n",
    "        gpu_id,\n",
    "        gpu_folds_info,\n",
    "        embeddings,\n",
    "        subset_sizes,\n",
    "        total_samples,\n",
    "        epsilon,\n",
    "        testing_mode,\n",
    "    ) = args\n",
    "\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "            device = f\"cuda:{gpu_id}\"\n",
    "        else:\n",
    "            if not testing_mode:\n",
    "                raise RuntimeError(\"GPU processing required but CUDA is not available\")\n",
    "            logger.warning(\n",
    "                \"Running in CPU mode for testing. Production use requires GPU acceleration.\"\n",
    "            )\n",
    "            device = \"cpu\"\n",
    "\n",
    "        results = []\n",
    "        for fold_idx, fold_indices in gpu_folds_info:\n",
    "            try:\n",
    "                logger.info(f\"Processing fold {fold_idx + 1} on GPU {gpu_id}\")\n",
    "\n",
    "                fold_embeddings = embeddings[fold_indices].to(device)\n",
    "\n",
    "                logger.info(f\"Computing similarity matrix for fold {fold_idx + 1}\")\n",
    "                max_sim_mat = compute_pairwise_dense(\n",
    "                    fold_embeddings,\n",
    "                    batch_size=50000,\n",
    "                    metric=\"cosine\",\n",
    "                    device=device,\n",
    "                    scaling=\"additive\",\n",
    "                )\n",
    "                similarity_matrix = max_sim_mat.cpu().numpy()\n",
    "\n",
    "                subsets = {}\n",
    "                ds_func = FacilityLocationFunction(\n",
    "                    n=similarity_matrix.shape[0],\n",
    "                    sijs=similarity_matrix,\n",
    "                    mode=\"dense\",\n",
    "                    separate_rep=False,\n",
    "                )\n",
    "\n",
    "                for size_spec in subset_sizes:\n",
    "                    if isinstance(size_spec, float):\n",
    "                        # Percentage-based selection\n",
    "                        budget = max(\n",
    "                            1, math.ceil(size_spec * similarity_matrix.shape[0])\n",
    "                        )\n",
    "                    else:\n",
    "                        # Absolute number-based selection\n",
    "                        budget = max(\n",
    "                            1,\n",
    "                            math.ceil(\n",
    "                                size_spec * (similarity_matrix.shape[0] / total_samples)\n",
    "                            ),\n",
    "                        )\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Selecting subset of size {budget} for fold {fold_idx + 1}\"\n",
    "                    )\n",
    "\n",
    "                    subset_result = ds_func.maximize(\n",
    "                        budget=budget,\n",
    "                        optimizer=\"LazierThanLazyGreedy\",\n",
    "                        epsilon=epsilon,\n",
    "                        stopIfZeroGain=False,\n",
    "                        stopIfNegativeGain=False,\n",
    "                        verbose=False,\n",
    "                    )\n",
    "\n",
    "                    subset_indices = [fold_indices[x[0]] for x in subset_result]\n",
    "                    subset_gains = [x[1] for x in subset_result]\n",
    "                    subsets[size_spec] = {\n",
    "                        \"indices\": subset_indices,\n",
    "                        \"gains\": subset_gains,\n",
    "                    }\n",
    "\n",
    "                results.append((fold_idx, subsets))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error processing fold {fold_idx + 1} on GPU {gpu_id}: {str(e)}\"\n",
    "                )\n",
    "                raise\n",
    "            finally:\n",
    "                # Cleanup - ADDED THIS SECTION\n",
    "                for var in [\"ds_func\", \"similarity_matrix\", \"fold_embeddings\"]:\n",
    "                    if var in locals():\n",
    "                        del locals()[var]\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_folds_with_gpu on GPU {gpu_id}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"✅ process_folds_with_gpu function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6bf0d",
   "metadata": {},
   "source": [
    "### 🎯 Two Ways to Run Subset Selection\n",
    "\n",
    "**✅ RECOMMENDED: Use Existing Embeddings (Fast)**\n",
    "- Function: `run_subset_selection_only()`\n",
    "- Use when: You've already run Notebook 2 and have embeddings\n",
    "- Benefits: Much faster (minutes vs hours), no redundant computation\n",
    "- What it does: Loads embeddings from Notebook 2 → Runs facility location → Saves subsets\n",
    "\n",
    "**🔄 Alternative: Full Pipeline (Slow)**\n",
    "- Function: `subset_datasets()`\n",
    "- Use when: Running standalone without Notebook 2, or need fresh embeddings\n",
    "- What it does: Generates embeddings → Runs facility location → Saves subsets\n",
    "- Note: This regenerates embeddings even if they exist!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ee2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_subset_selection_only(\n",
    "    embeddings_file: str,\n",
    "    dataset,\n",
    "    output_dir: str,\n",
    "    subset_sizes: List[Union[int, float]],\n",
    "    dataset_name: str = \"combined_cut_50x\", #Change it according to your dataset\n",
    "    num_folds: int = 25,\n",
    "    epsilon: float = 160.0,\n",
    "    num_gpus: int = None,\n",
    "    testing_mode: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run ONLY subset selection using pre-computed embeddings from Notebook 2.\n",
    "    \n",
    "    This function is designed for the notebook workflow where embeddings \n",
    "    are already generated. It skips the embedding generation step entirely.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_file: Path to the embeddings.h5 file from Notebook 2\n",
    "        dataset: The dataset loaded in Notebook 1\n",
    "        output_dir: Where to save subset results\n",
    "        subset_sizes: List of subset sizes (floats for %, ints for absolute)\n",
    "        dataset_name: Name of the dataset (for output files)\n",
    "        num_folds: Number of folds for facility location\n",
    "        epsilon: Epsilon parameter for the optimizer\n",
    "        num_gpus: Number of GPUs to use (auto-detected if None)\n",
    "        testing_mode: Enable testing mode for CPU fallback\n",
    "    \"\"\"\n",
    "    # Verify embeddings file exists\n",
    "    if not os.path.exists(embeddings_file):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Embeddings file not found: {embeddings_file}\\n\"\n",
    "            \"Please run Notebook 2 (embedding_generation.ipynb) first!\"\n",
    "        )\n",
    "    \n",
    "    # Auto-detect GPUs if not specified\n",
    "    if num_gpus is None:\n",
    "        num_gpus = get_default_num_gpus(testing_mode=testing_mode)\n",
    "    \n",
    "    logger.info(f\"📁 Using existing embeddings from: {embeddings_file}\")\n",
    "    logger.info(f\"🎯 Dataset: {dataset_name} with {len(dataset)} samples\")\n",
    "    logger.info(f\"📊 Subset sizes: {subset_sizes}\")\n",
    "    logger.info(f\"🔧 Folds: {num_folds}, Epsilon: {epsilon}\")\n",
    "    \n",
    "    # Create configuration (minimal, just for subset selection)\n",
    "    basic_config = BasicConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_folds=num_folds,\n",
    "        epsilon=epsilon,\n",
    "        combine_files=False,\n",
    "    )\n",
    "    \n",
    "    # Validate epsilon for dataset size\n",
    "    basic_config.validate_epsilon_for_dataset_size(len(dataset))\n",
    "    \n",
    "    # Create system config (num_gpus is auto-detected in __post_init__)\n",
    "    system_config = SystemConfig(\n",
    "        testing_mode=testing_mode,\n",
    "    )\n",
    "    \n",
    "    # Override num_gpus if specified\n",
    "    if num_gpus is not None:\n",
    "        system_config.num_gpus = num_gpus\n",
    "    \n",
    "    # Create minimal config for subset selection only\n",
    "    config = ProcessingConfig(\n",
    "        input_files=[],  # Not needed for this path\n",
    "        subset_sizes=subset_sizes,\n",
    "        basic=basic_config,\n",
    "        encoder=EncoderConfig(testing_mode=testing_mode),\n",
    "        template=TemplateConfig(),\n",
    "        system=system_config,\n",
    "    )\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = DataProcessor(config)\n",
    "    \n",
    "    try:\n",
    "        # Load embeddings\n",
    "        logger.info(\"📥 Loading embeddings...\")\n",
    "        with h5py.File(embeddings_file, \"r\") as f:\n",
    "            embeddings_data = f[\"embeddings\"][:]\n",
    "            if embeddings_data.size == 0:\n",
    "                raise ValueError(\"Embeddings file is empty!\")\n",
    "            embeddings = torch.tensor(embeddings_data, dtype=torch.float32)\n",
    "        \n",
    "        logger.info(f\"✅ Loaded embeddings: shape {embeddings.shape}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        dataset_output_dir = os.path.join(output_dir, dataset_name)\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Run subset selection\n",
    "        logger.info(\"🎯 Running subset selection...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        subsets = processor.select_subsets(dataset_name, embeddings)\n",
    "        \n",
    "        selection_time = time.time() - start_time\n",
    "        \n",
    "        # Save subsets to files\n",
    "        logger.info(\"💾 Saving selected subsets...\")\n",
    "        for size_spec, indices in subsets.items():\n",
    "            subset_data = dataset.select(indices)\n",
    "            subset_name = processor.get_subset_name(size_spec, len(indices))\n",
    "            \n",
    "            output_file = os.path.join(\n",
    "                dataset_output_dir,\n",
    "                f\"{dataset_name}_{subset_name}_subset.jsonl\",\n",
    "            )\n",
    "            \n",
    "            processor._save_subset(subset_data, output_file, \"dummy.jsonl\")\n",
    "            logger.info(f\"✅ Saved {len(indices)} samples to {output_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"✅ SUBSET SELECTION COMPLETED!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"⏱️  Selection time: {selection_time / 60:.2f} minutes\")\n",
    "        print(f\"📊 Created {len(subsets)} subset(s):\")\n",
    "        for size_spec, indices in subsets.items():\n",
    "            subset_name = processor.get_subset_name(size_spec, len(indices))\n",
    "            print(f\"   • {subset_name}: {len(indices)} samples\")\n",
    "        print(f\"💾 Output directory: {dataset_output_dir}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during subset selection: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ run_subset_selection_only() function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226b657",
   "metadata": {},
   "source": [
    "### Extend DataProcessor with Subset Selection\n",
    "\n",
    "Adds the `select_subsets()` method to the DataProcessor class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subsets(self, dataset_name: str, embeddings: torch.Tensor) -> Dict[Union[int, float], List[int]]:\n",
    "    \"\"\"\n",
    "    Perform diverse subset selection using facility location with multi-GPU support.\n",
    "    \n",
    "    This method implements a three-phase approach to select representative subsets from \n",
    "    embeddings using submodular optimization. It supports both percentage-based and \n",
    "    absolute count specifications for subset sizes.\n",
    "    \n",
    "    **Process Overview:**\n",
    "    \n",
    "    Phase 1 - Create Folds:\n",
    "        - Randomly shuffles sample indices\n",
    "        - Splits into N folds with roughly equal sizes\n",
    "        - Distributes folds across available GPUs\n",
    "    \n",
    "    Phase 2 - Process Folds (Parallel/Serial):\n",
    "        - Testing Mode: Processes folds serially (one at a time)\n",
    "        - Production Mode: Processes folds in parallel using multiprocessing.Pool\n",
    "        - Each GPU runs facility location algorithm on its assigned folds\n",
    "    \n",
    "    Phase 3 - Aggregate Results:\n",
    "        - Collects selected indices and gain scores from all folds\n",
    "        - Sorts by gain scores (highest first)\n",
    "        - Selects top N samples for each target subset size\n",
    "        - Saves metadata to .npz files\n",
    "    \n",
    "    **Selection Modes:**\n",
    "        - Percentage: Float values (0.1 = 10%, 0.05 = 5% of dataset)\n",
    "        - Absolute: Integer values (100 = exactly 100 samples)\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset, used for output file naming.\n",
    "        embeddings (torch.Tensor): Embedding tensor of shape (num_samples, embedding_dim).\n",
    "            Contains the vector representations of all samples in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[Union[int, float], List[int]]: Dictionary mapping subset size specifications \n",
    "            to lists of selected sample indices. Keys are the original size_spec values \n",
    "            (floats for percentages, ints for absolute counts), and values are lists of \n",
    "            integer indices representing the selected samples.\n",
    "    \n",
    "    Output Files:\n",
    "        Creates metadata files in the format:\n",
    "        `{output_dir}/{dataset_name}_fl_{num_folds}_partitions_{subset_name}_metadata.npz`\n",
    "        \n",
    "        Each file contains:\n",
    "            - indices: numpy array of selected sample indices\n",
    "            - gains: numpy array of corresponding gain scores from facility location\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(embeddings))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    fold_size = len(embeddings) // self.config.basic.num_folds\n",
    "    remainder = len(embeddings) % self.config.basic.num_folds\n",
    "\n",
    "    folds = []\n",
    "    start_idx = 0\n",
    "    for i in range(self.config.basic.num_folds):\n",
    "        extra = 1 if i < remainder else 0\n",
    "        end_idx = start_idx + fold_size + extra\n",
    "        folds.append(indices[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "\n",
    "    gpu_assignments = []\n",
    "    folds_per_gpu = self.config.basic.num_folds // self.config.system.num_gpus\n",
    "    extra_folds = self.config.basic.num_folds % self.config.system.num_gpus\n",
    "\n",
    "    start_fold = 0\n",
    "    for gpu_id in range(self.config.system.num_gpus):\n",
    "        num_folds_this_gpu = folds_per_gpu + (1 if gpu_id < extra_folds else 0)\n",
    "        end_fold = start_fold + num_folds_this_gpu\n",
    "        gpu_folds_info = [\n",
    "            (fold_idx, folds[fold_idx]) for fold_idx in range(start_fold, end_fold)\n",
    "        ]\n",
    "\n",
    "        gpu_assignments.append(\n",
    "            (\n",
    "                gpu_id,\n",
    "                gpu_folds_info,\n",
    "                embeddings,\n",
    "                self.config.subset_sizes,\n",
    "                len(embeddings),\n",
    "                self.config.basic.epsilon,\n",
    "                self.config.system.testing_mode,\n",
    "            )\n",
    "        )\n",
    "        start_fold = end_fold\n",
    "\n",
    "    # Use serial processing in testing mode (notebook-friendly)\n",
    "    if self.config.system.testing_mode or self.config.system.num_gpus == 1:\n",
    "        logger.info(\"Processing folds serially (testing mode or single GPU)\")\n",
    "        gpu_results = []\n",
    "        for args in gpu_assignments:\n",
    "            result = process_folds_with_gpu(args)\n",
    "            gpu_results.append(result)\n",
    "    else:\n",
    "        logger.info(f\"Processing folds in parallel with {self.config.system.num_gpus} workers\")\n",
    "        with Pool(processes=self.config.system.num_gpus) as pool:\n",
    "            gpu_results = pool.map(process_folds_with_gpu, gpu_assignments)\n",
    "\n",
    "    all_results = []\n",
    "    for gpu_result in gpu_results:\n",
    "        all_results.extend(gpu_result)\n",
    "\n",
    "    class SubsetData(TypedDict):\n",
    "        indices: List[int]\n",
    "        gains: List[float]\n",
    "\n",
    "    combined_subsets: Dict[Union[int, float], SubsetData] = {\n",
    "        size: {\"indices\": [], \"gains\": []} for size in self.config.subset_sizes\n",
    "    }\n",
    "\n",
    "    for fold_idx, result in all_results:\n",
    "        for size in self.config.subset_sizes:\n",
    "            combined_subsets[size][\"indices\"].extend(result[size][\"indices\"])\n",
    "            combined_subsets[size][\"gains\"].extend(result[size][\"gains\"])\n",
    "\n",
    "    base_name = dataset_name\n",
    "    subsets = {}\n",
    "\n",
    "    for size_spec in self.config.subset_sizes:\n",
    "        actual_size = self.calculate_subset_size(len(embeddings), size_spec)\n",
    "        logger.info(f\"Actual subset size: {actual_size}\")\n",
    "        sorted_indices_gains = sorted(\n",
    "            zip(\n",
    "                combined_subsets[size_spec][\"indices\"],\n",
    "                combined_subsets[size_spec][\"gains\"],\n",
    "            ),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True,\n",
    "        )[:actual_size]\n",
    "\n",
    "        sorted_indices = [x[0] for x in sorted_indices_gains]\n",
    "        sorted_gains = [x[1] for x in sorted_indices_gains]\n",
    "\n",
    "        subset_name = self.get_subset_name(size_spec, actual_size)\n",
    "        metadata_file = os.path.join(\n",
    "            self.config.basic.output_dir,\n",
    "            f\"{base_name}_fl_{self.config.basic.num_folds}_partitions_{subset_name}_metadata.npz\",\n",
    "        )\n",
    "\n",
    "        np.savez(metadata_file, indices=sorted_indices, gains=sorted_gains)\n",
    "        logger.info(f\"Saved metadata to {metadata_file}\")\n",
    "        subsets[size_spec] = sorted_indices\n",
    "\n",
    "    return subsets\n",
    "\n",
    "\n",
    "# Add the method to DataProcessor class\n",
    "DataProcessor.select_subsets = select_subsets\n",
    "\n",
    "print(\"✅ select_subsets() method added to DataProcessor!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e89212",
   "metadata": {},
   "source": [
    "### Add File Processing Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_subset(self, subset_data, output_file: str, input_file: str):\n",
    "    \"\"\"\n",
    "    Save subset data to file in appropriate format.\n",
    "    \n",
    "    Args:\n",
    "        subset_data: The dataset subset to save\n",
    "        output_file (str): Output file path\n",
    "        input_file (str): Original input file path (for determining format)\n",
    "    \"\"\"\n",
    "    extension = input_file.split(\".\")[-1]\n",
    "    if extension in [\"json\", \"jsonl\"]:\n",
    "        subset_data.to_json(output_file, orient=\"records\", lines=True)\n",
    "    elif extension == \"csv\":\n",
    "        subset_data.to_csv(output_file, index=False)\n",
    "    elif extension == \"parquet\":\n",
    "        subset_data.to_parquet(output_file)\n",
    "    \n",
    "    logger.info(f\"Saved subset to {output_file}\")\n",
    "\n",
    "\n",
    "def _process_single_dataset(\n",
    "    self,\n",
    "    dataset,\n",
    "    dataset_name: str,\n",
    "    output_dir: str,\n",
    "    input_file: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single dataset (either combined or individual).\n",
    "    \n",
    "    This function orchestrates the complete pipeline for a single dataset:\n",
    "    1. Validates epsilon parameter\n",
    "    2. Generates or loads embeddings\n",
    "    3. Selects subsets using Facility Location\n",
    "    4. Saves selected subsets to files\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to process\n",
    "        dataset_name: Name of the dataset\n",
    "        output_dir: Output directory for results\n",
    "        input_file: Original input file path (for determining format)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate epsilon based on dataset size\n",
    "        self.config.basic.validate_epsilon_for_dataset_size(len(dataset))\n",
    "\n",
    "        # Create dataset-specific output directory\n",
    "        dataset_output_dir = os.path.join(output_dir, dataset_name)\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Generating embeddings for {dataset_name}\")\n",
    "        embedding_file = self.generate_embeddings(\n",
    "            dataset, os.path.join(dataset_output_dir, \"embeddings\")\n",
    "        )\n",
    "\n",
    "        logger.info(\"Loading embeddings for subset selection\")\n",
    "        with h5py.File(embedding_file, \"r\") as f:\n",
    "            embeddings_data = f[\"embeddings\"][:]\n",
    "            if embeddings_data.size == 0:\n",
    "                logger.warning(\n",
    "                    f\"No embeddings generated for dataset {dataset_name}, skipping subset selection\"\n",
    "                )\n",
    "                return\n",
    "            embeddings = torch.tensor(embeddings_data, dtype=torch.float32)\n",
    "\n",
    "        logger.info(\"Selecting subsets\")\n",
    "        subsets = self.select_subsets(dataset_name, embeddings)\n",
    "\n",
    "        logger.info(\"Saving subsets\")\n",
    "        for size_spec, indices in subsets.items():\n",
    "            subset_data = dataset.select(indices)\n",
    "            subset_name = self.get_subset_name(size_spec, len(indices))\n",
    "\n",
    "            # Create subset filename with dataset name\n",
    "            output_file = os.path.join(\n",
    "                dataset_output_dir,\n",
    "                f\"{dataset_name}_{subset_name}_subset.{input_file.split('.')[-1]}\",\n",
    "            )\n",
    "\n",
    "            self._save_subset(subset_data, output_file, input_file)\n",
    "            logger.info(\n",
    "                f\"Saved subset with {len(indices)} samples to {output_file}\"\n",
    "            )\n",
    "\n",
    "        # Clean up resources\n",
    "        del dataset, embeddings\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing dataset {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_files(self, input_files: List[str], output_dir: str):\n",
    "    \"\"\"\n",
    "    Process multiple input files with support for both combined and separate processing.\n",
    "    \n",
    "    Args:\n",
    "        input_files (List[str]): List of input files to process\n",
    "        output_dir (str): Output directory for results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if self.config.basic.combine_files:\n",
    "            # Process combined datasets\n",
    "            logger.info(\"Processing combined datasets...\")\n",
    "            dataset = self.load_and_combine_datasets(input_files)\n",
    "            dataset_name = \"combined_dataset\"\n",
    "\n",
    "            # Process combined dataset\n",
    "            self._process_single_dataset(\n",
    "                dataset, dataset_name, output_dir, input_files[0]\n",
    "            )\n",
    "        else:\n",
    "            # Process each dataset separately\n",
    "            logger.info(\"Processing datasets separately...\")\n",
    "            for input_file in input_files:\n",
    "                dataset = self.load_and_combine_datasets([input_file])\n",
    "                dataset_name = self.get_dataset_name(input_file)\n",
    "                logger.info(f\"Processing dataset: {dataset_name}\")\n",
    "                self._process_single_dataset(\n",
    "                    dataset, dataset_name, output_dir, input_file\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Add all methods to DataProcessor class\n",
    "DataProcessor._save_subset = _save_subset\n",
    "DataProcessor._process_single_dataset = _process_single_dataset\n",
    "DataProcessor.process_files = process_files\n",
    "\n",
    "print(\"✅ Additional methods added to DataProcessor!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c575d7",
   "metadata": {},
   "source": [
    "### Main Wrapper Function: subset_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_datasets(\n",
    "    input_files: List[str],\n",
    "    subset_sizes: List[Union[int, float]],\n",
    "    testing_mode: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create subsets of datasets using facility location for diverse subset selection.\n",
    "    \n",
    "    This is the main entry point from the original codebase. It creates configuration,\n",
    "    initializes the processor, and runs the complete pipeline.\n",
    "    \n",
    "    Args:\n",
    "        input_files: List of input files to process\n",
    "        subset_sizes: List of subset sizes (floats 0-1 for percentage, ints for absolute count)\n",
    "        testing_mode: If True, allows CPU usage (for testing only)\n",
    "        **kwargs: Additional configuration parameters (e.g., output_dir, epsilon, num_folds)\n",
    "    \"\"\"\n",
    "    # Get system's available GPU count\n",
    "    available_gpus = get_default_num_gpus(testing_mode=testing_mode)\n",
    "\n",
    "    # Create configuration groups\n",
    "    basic_config = BasicConfig()\n",
    "    encoder_config = EncoderConfig(testing_mode=testing_mode)\n",
    "    template_config = TemplateConfig()\n",
    "    system_config = SystemConfig(testing_mode=testing_mode)\n",
    "\n",
    "    # Update configuration groups from kwargs\n",
    "    for key, value in kwargs.items():\n",
    "        if hasattr(basic_config, key):\n",
    "            setattr(basic_config, key, value)\n",
    "        elif hasattr(encoder_config, key):\n",
    "            setattr(encoder_config, key, value)\n",
    "        elif hasattr(template_config, key):\n",
    "            setattr(template_config, key, value)\n",
    "        elif hasattr(system_config, key):\n",
    "            setattr(system_config, key, value)\n",
    "\n",
    "    # Ensure num_gpus doesn't exceed available GPUs\n",
    "    if system_config.num_gpus > available_gpus:\n",
    "        logger.warning(\n",
    "            f\"Requested {system_config.num_gpus} GPUs but only {available_gpus} available. \"\n",
    "            f\"Falling back to using {available_gpus} GPUs.\"\n",
    "        )\n",
    "        system_config.num_gpus = available_gpus\n",
    "\n",
    "    # Create configuration\n",
    "    config = ProcessingConfig(\n",
    "        input_files=input_files,\n",
    "        subset_sizes=subset_sizes,\n",
    "        basic=basic_config,\n",
    "        encoder=encoder_config,\n",
    "        template=template_config,\n",
    "        system=system_config,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Processing configuration: {config}\")\n",
    "        processor = DataProcessor(config)\n",
    "        processor.process_files(input_files, config.basic.output_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"✅ subset_datasets() function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88e141",
   "metadata": {},
   "source": [
    "## 🚀 Execute Subset Selection\n",
    "\n",
    "**Choose ONE of the two options below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================== OPTION 1: USE EXISTING EMBEDDINGS (RECOMMENDED) ====================\n",
    "##\n",
    "## ✅ This is the FAST approach - it uses embeddings already generated in Notebook 2\n",
    "## ⚡ Takes only minutes (vs hours for full pipeline)\n",
    "## 💾 Loads embeddings from: data-preparation-and-config/output/embeddings/embeddings.h5\n",
    "\n",
    "## **Requirements:**\n",
    "## - Notebook 2 must be run first\n",
    "## - Embeddings file must exist at the specified path\n",
    "##\n",
    "## Uncomment the code below to run:\n",
    "\n",
    "_option_1_success = False\n",
    "try:\n",
    "    run_subset_selection_only(\n",
    "        embeddings_file='../../assets/subset-selection/outputs/embeddings/embeddings.h5',  # From Notebook 2\n",
    "        dataset=dataset,  # Loaded from Notebook 1\n",
    "        output_dir=\"../../assets/subset-selection/outputs\", # change this to your desired output directory\n",
    "        subset_sizes=[0.1, 0.05],  # 10% and 5% subsets\n",
    "        dataset_name='combined_cut_50x',\n",
    "        num_folds=25,\n",
    "        epsilon=0.1,\n",
    "        testing_mode=True,\n",
    "    )\n",
    "    _option_1_success = True\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "## ==================== OPTION 2: FULL PIPELINE FROM SCRATCH (SLOW) ====================\n",
    "##\n",
    "## 🔄 This regenerates embeddings even if they already exist\n",
    "## ⏰ Takes hours for large datasets\n",
    "## 🎯 Use this only if running notebook 3 independently without notebook 2\n",
    "##\n",
    "## Uncomment the code below to run:\n",
    "\n",
    "_option_2_success = False\n",
    "# try:\n",
    "#     subset_datasets(\n",
    "#         input_files=['data/combined_cut_50x.jsonl'],\n",
    "#         subset_sizes=[0.1, 0.05],  # 10% and 5% subsets\n",
    "#         testing_mode=True,\n",
    "#         output_dir='data/output',\n",
    "#         num_folds=25,\n",
    "#         epsilon=0.1\n",
    "#     )\n",
    "#     _option_2_success = True\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "\n",
    "## ==================== EXECUTION STATUS ====================\n",
    "# Smart status reporting based on what actually ran\n",
    "\n",
    "if _option_1_success or _option_2_success:\n",
    "    # Success case - at least one option ran successfully\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ SUBSET SELECTION WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    if _option_1_success:\n",
    "        print(\"📍 Executed: Option 1 (Using existing embeddings)\")\n",
    "    if _option_2_success:\n",
    "        print(\"📍 Executed: Option 2 (Full pipeline from scratch)\")\n",
    "    \n",
    "else:\n",
    "    # No option ran successfully or both are commented out\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"⚠️  NO EXECUTION OPTION SELECTED OR ALL OPTIONS FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n📋 Current Status:\")\n",
    "    print(\"   • Option 1: Active but failed (check errors above)\" if not _option_1_success and '_option_1_success' in locals() else \"   • Option 1: Commented out\")\n",
    "    print(\"   • Option 2: Commented out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b22329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
