{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f3ba5f",
   "metadata": {},
   "source": [
    "# Data Processing: Chunking with HybridChunker\n",
    "\n",
    "This notebook demonstrates how to use the **hybrid (token-based)** [Docling](https://docling-project.github.io/docling/) chunking strategy to split documents into smaller, semantically meaningful pieces. These chunks are essential for Retrieval-Augmented Generation (RAG) workflows.\n",
    "\n",
    "The `HybridChunker` combines structural, rule-based chunking with token-aware splitting. This notebook will walk you through its main parameters to show how different settings affect the size and content of the resulting document chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be80a0f",
   "metadata": {},
   "source": [
    "## 📦 Installation\n",
    "\n",
    "Install the necessary packages into this notebook environment. We need `docling` for chunking and `transformers` for tokenization. Run this once per session. If you restart the kernel, re-run this cell before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "246cabfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq docling transformers\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253ca8e",
   "metadata": {},
   "source": [
    "## 🔧 Configuration\n",
    "\n",
    "### Set file to be chunked\n",
    "\n",
    "Set the source file to be chunked. We will download a sample `docling` JSON document from a public URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c24f2872",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# URL to the raw docling JSON file on GitHub\n",
    "file_to_chunk = \"https://raw.githubusercontent.com/docling-project/docling/refs/tags/v2.55.1/tests/data/groundtruth/docling_v2/2206.01062.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233a6d1",
   "metadata": {},
   "source": [
    "### Set output directory\n",
    "\n",
    "Choose where to save results. This notebook creates the folder if it doesn’t exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c1e784b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"hybrid-chunking-demonstration/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648fc65",
   "metadata": {},
   "source": [
    "### Configure chunking strategies\n",
    "\n",
    "Next, we set up the different chunking strategies we want to demonstrate. The cell below contains three configurations:\n",
    "\n",
    "1.  **`default_chunker`**:A `HybridChunker` initialized without an explicit tokenizer or token limit. It automatically uses a default tokenizer (e.g., a sentence-embedding model's tokenizer) and its derived default token limit (typically 512 tokens). By default, it aggressively merges adjacent small structural chunks (like short paragraphs) to create a larger, contextually richer chunk, stopping when it reaches the token limit as the `merge_peers` is True by default. It also splits any individual structural element that exceeds token limit.\n",
    "2.  **`no_merge_chunker`**: A `HybridChunker` with `merge_peers=False`. This prevents the merging of adjacent chunks, resulting in smaller chunks that strictly follow the document's structure.Note, this still splits any individual structural element that exceeds token limit.\n",
    "2.  **`custom_chunker`**: A `HybridChunker` configured with a tokenizer of choice and a `max_tokens` limit.\n",
    "For additional customization, check the [official documentation](https://docling-project.github.io/docling/concepts/chunking/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "adf0d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Default HybridChunker (no tokenizer)\n",
    "default_chunker = HybridChunker()\n",
    "\n",
    "# Configure the tokenizer for the next runs\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 128\n",
    "hf_tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "# 2. HybridChunker with tokenizer but merge_peers=False\n",
    "no_merge_chunker = HybridChunker(\n",
    "    merge_peers=False,\n",
    ")\n",
    "# 3. HybridChunker with a tokenizer and max_tokens\n",
    "custom_chunker = HybridChunker(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    # merge_peers is True by default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ff69e",
   "metadata": {},
   "source": [
    "### Load the docling JSON into a Python object\n",
    "The `file_to_chunk` is a JSON docling document generated from a pdf which is an arXiv paper about large language models, which provides a good mix of headings, paragraphs, and other structural elements that will help demonstrate the different chunking behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e463c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 11:58:42,211 - INFO - detected formats: [<InputFormat.JSON_DOCLING: 'json_docling'>]\n",
      "2025-10-09 11:58:42,264 - INFO - Going to convert document batch...\n",
      "2025-10-09 11:58:42,265 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-10-09 11:58:42,266 - INFO - Processing document 2206.01062.json\n",
      "2025-10-09 11:58:42,269 - INFO - Finished converting document 2206.01062.json in 0.14 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded: 2206.01062\n"
     ]
    }
   ],
   "source": [
    "# Convert to a docling document\n",
    "doc = DocumentConverter().convert(source=file_to_chunk)\n",
    "print(f\"Document loaded: {doc.document.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c71c5",
   "metadata": {},
   "source": [
    "## 🧪Experiment: Comparing Chunking Strategies\n",
    "\n",
    "Let's compare how each chunking strategy affects the document:\n",
    "\n",
    "1. **Default HybridChunker**: Default settings for tokenizer, max_tokens and merge_peers(True)\n",
    "2. **No-Merge Chunker**: set merge_peers=False\n",
    "3. **Custom Chunker**: Using tokenizer with max_tokens=128, merge_peers=True\n",
    "\n",
    "For each strategy, we'll show:\n",
    "- Total number of chunks created\n",
    "- Average chunk size (in tokens)\n",
    "- Sample chunks with their token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "393654ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function created for analyzing chunks\n"
     ]
    }
   ],
   "source": [
    "def analyze_chunks(chunker, doc, num_samples=3):\n",
    "    \"\"\"Analyze chunks produced by a chunker, including token counts and statistics.\"\"\"\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    \n",
    "    # Get token counts if we have a tokenizer\n",
    "    token_counts = []\n",
    "    if chunker.tokenizer:\n",
    "        for chunk in chunks:\n",
    "            text = chunker.contextualize(chunk=chunk)\n",
    "            try:\n",
    "                count = len(chunker.tokenizer.tokenizer.encode(text))\n",
    "                token_counts.append(count)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not count tokens for chunk: {e}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "    if token_counts:\n",
    "        avg_tokens = sum(token_counts) / len(token_counts)\n",
    "        print(f\"Average tokens per chunk: {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample chunks:\")\n",
    "    samples = random.sample(chunks, min(len(chunks), num_samples))\n",
    "    for i, chunk in enumerate(samples, 1):\n",
    "        text = chunker.contextualize(chunk=chunk)\n",
    "        print(f\"\\nChunk {i}:\")\n",
    "        if chunker.tokenizer:\n",
    "            try:\n",
    "                count = len(chunker.tokenizer.tokenizer.encode(text))\n",
    "                print(f\"Token count: {count}\")\n",
    "            except Exception:\n",
    "                print(\"Token count: N/A\")\n",
    "        print(\"-\" * 40)\n",
    "        print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"Helper function created for analyzing chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c721",
   "metadata": {},
   "source": [
    "### 1. Default HybridChunker\n",
    "\n",
    "First, let's see how the HybridChunker with default settinsg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94e75003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2999 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running default HybridChunker (no tokenizer)...\n",
      "\n",
      "Total chunks: 75\n",
      "Average tokens per chunk: 196.7\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Token count: 253\n",
      "----------------------------------------\n",
      "4 ANNOTATION CAMPAIGN\n",
      "45.00. Text, triple inter-annotator mAP @0.5-0.95 (%).All = 84-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Fin = 81-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Man ...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Token count: 258\n",
      "----------------------------------------\n",
      "4 ANNOTATION CAMPAIGN\n",
      "% of Total.Test = 15.77. Section-header, % of Total.Val = 12.85. Section-header, triple inter-annotator mAP @0.5-0.95 (%).All = 83-84. Section-header, triple inter-annotator mAP ...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Token count: 59\n",
      "----------------------------------------\n",
      "Baselines for Object Detection\n",
      "document.\n",
      "\n",
      "Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obta...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Running default HybridChunker (no tokenizer)...\")\n",
    "analyze_chunks(default_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a07aa",
   "metadata": {},
   "source": [
    "### 2. Disabling Peer Merging\n",
    "\n",
    "Let's see what happens when we disable peer merging. This will prevent the chunker from combining small adjacent chunks, even if they would fit within our token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fcacdd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2999 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HybridChunker (with merge_peers=False)...\n",
      "\n",
      "Total chunks: 121\n",
      "Average tokens per chunk: 124.9\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Token count: 40\n",
      "----------------------------------------\n",
      "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Token count: 134\n",
      "----------------------------------------\n",
      "1 INTRODUCTION\n",
      "- (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.\n",
      "- (2) Large Layout Variability : ...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Token count: 250\n",
      "----------------------------------------\n",
      "4 ANNOTATION CAMPAIGN\n",
      "Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recu...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Running HybridChunker (with merge_peers=False)...\")\n",
    "analyze_chunks(no_merge_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f547fef",
   "metadata": {},
   "source": [
    "### 3. Custom Chunking\n",
    "\n",
    "Now let's set the tokenizer and max_tokens on the HybridChunker.Since we are setting max_tokens to a smaller value, you should see increased number of chunks. We are setting the merge_peers to True so this will merge chunks to stay under the token limit while trying to preserve document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "31d5b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2999 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  HybridChunker with custom tokenizer and max_tokens...\n",
      "\n",
      "Total chunks: 142\n",
      "Average tokens per chunk: 106.8\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Token count: 130\n",
      "----------------------------------------\n",
      "4 ANNOTATION CAMPAIGN\n",
      "@0.5-0.95 (%).Ten = 96-98. Page-header, Count = 58022. Page-header, % of Total.Train = 5.10. Page-header, % of Total.Test = 6.70. Page-header, % of Total.Val = 5.06. Page-header,...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Token count: 125\n",
      "----------------------------------------\n",
      "Dataset Comparison\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "Table 5: Prediction Performance (mAP@0.5-0.95) of ...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Token count: 30\n",
      "----------------------------------------\n",
      "3 THE DOCLAYNET DATASET\n",
      "JSON). All additional files are linked to the primary page images by their matching filenames.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Running  HybridChunker with custom tokenizer and max_tokens...\")\n",
    "analyze_chunks(custom_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9703f84",
   "metadata": {},
   "source": [
    "## 🍩 Additional Resources\n",
    "\n",
    "For more information about Docling and its features:\n",
    "\n",
    "- [Docling Documentation](https://docling-project.github.io/docling/)\n",
    "- [Open Data Hub Data Processing Examples](https://github.com/opendatahub-io/odh-data-processing)\n",
    "\n",
    "### Any Feedback?\n",
    "\n",
    "We'd love to hear if you have any feedback on this or any other notebook in this series! Please [open an issue](https://github.com/opendatahub-io/odh-data-processing/issues) and help us improve our demos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
