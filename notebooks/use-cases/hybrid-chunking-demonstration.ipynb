{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f3ba5f",
   "metadata": {},
   "source": [
    "# Data Processing: Chunking with HybridChunker\n",
    "\n",
    "This notebook demonstrates how to use the **hybrid (token-based)** [Docling](https://docling-project.github.io/docling/) chunking strategy to split documents into smaller, semantically meaningful pieces. These chunks are essential for Retrieval-Augmented Generation (RAG) workflows.\n",
    "\n",
    "The `HybridChunker` combines structural, rule-based chunking with token-aware splitting. This notebook will walk you through its main parameters to show how different settings affect the size and content of the resulting document chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be80a0f",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "Install the necessary packages into this notebook environment. We need `docling` for chunking and `transformers` for tokenization. Run this once per session. If you restart the kernel, re-run this cell before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq docling transformers\n",
    "\n",
    "from pathlib import Path\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253ca8e",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "\n",
    "### Set file to be chunked\n",
    "\n",
    "Set the source file to be chunked. We will download a sample `docling` JSON document from a public URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f2872",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# URL to the raw docling JSON file on GitHub\n",
    "file_to_chunk = \"https://raw.githubusercontent.com/docling-project/docling/refs/tags/v2.55.1/tests/data/groundtruth/docling_v2/2206.01062.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233a6d1",
   "metadata": {},
   "source": [
    "### Set output directory\n",
    "\n",
    "Choose where to save results. This notebook creates the folder if it doesn‚Äôt exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e784b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"hybrid-chunking-demonstration/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648fc65",
   "metadata": {},
   "source": [
    "### Configure chunking strategies\n",
    "\n",
    "Next, we set up the different chunking strategies we want to demonstrate. The cell below contains three configurations:\n",
    "\n",
    "1.  **`default_chunker`**: A `HybridChunker` initialized without an explicit tokenizer or token limit. It automatically uses a default tokenizer (e.g., a sentence-embedding model's tokenizer) and its derived default token limit (typically 512 tokens). By default, it aggressively merges adjacent small structural chunks (like short paragraphs) to create a larger, contextually richer chunk, stopping when it reaches the token limit as the `merge_peers` is True by default. It also splits any individual structural element that exceeds token limit.\n",
    "2.  **`no_merge_chunker`**: A `HybridChunker` with `merge_peers=False`. This prevents the merging of adjacent chunks, resulting in smaller chunks that strictly follow the document's structure. Note, this still splits any individual structural element that exceeds token limit.\n",
    "3.  **`custom_chunker`**: A `HybridChunker` configured with a tokenizer of choice and a `max_tokens` limit.\n",
    "For additional customization, check the [official documentation](https://docling-project.github.io/docling/concepts/chunking/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Default HybridChunker (no tokenizer)\n",
    "default_chunker = HybridChunker()\n",
    "\n",
    "# Configure the tokenizer for the next runs\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 128\n",
    "hf_tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "# 2. HybridChunker with tokenizer but merge_peers=False\n",
    "no_merge_chunker = HybridChunker(\n",
    "    merge_peers=False,\n",
    ")\n",
    "# 3. HybridChunker with a tokenizer and max_tokens\n",
    "custom_chunker = HybridChunker(\n",
    "    tokenizer=hf_tokenizer,\n",
    "    # merge_peers is True by default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ff69e",
   "metadata": {},
   "source": [
    "### Load the docling JSON into a Python object\n",
    "The `file_to_chunk` is a JSON docling document generated from a pdf which is an arXiv paper about large language models, which provides a good mix of headings, paragraphs, and other structural elements that will help demonstrate the different chunking behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a docling document\n",
    "doc = DocumentConverter().convert(source=file_to_chunk)\n",
    "print(f\"Document loaded: {doc.document.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c71c5",
   "metadata": {},
   "source": [
    "## üß™Experiment: Comparing Chunking Strategies\n",
    "\n",
    "Let's compare how each chunking strategy affects the document:\n",
    "\n",
    "1. **Default HybridChunker**: Default settings for tokenizer, max_tokens and merge_peers(True)\n",
    "2. **No-Merge Chunker**: set merge_peers=False\n",
    "3. **Custom Chunker**: Using tokenizer with max_tokens=128, merge_peers=True\n",
    "\n",
    "For each strategy, we'll show:\n",
    "- Total number of chunks created\n",
    "- Average chunk size (in tokens)\n",
    "- Sample chunks with their token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunker, doc, num_samples=3):\n",
    "    \"\"\"Analyze chunks produced by a chunker, including token counts and statistics.\"\"\"\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    \n",
    "    # Get token counts if we have a tokenizer\n",
    "    token_counts = []\n",
    "    if chunker.tokenizer:\n",
    "        for chunk in chunks:\n",
    "            text = chunker.contextualize(chunk=chunk)\n",
    "            try:\n",
    "                count = len(chunker.tokenizer.tokenizer.encode(text))\n",
    "                token_counts.append(count)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not count tokens for chunk: {e}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "    if token_counts:\n",
    "        avg_tokens = sum(token_counts) / len(token_counts)\n",
    "        print(f\"Average tokens per chunk: {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Show first num_samples samples from the chunks\n",
    "    print(\"\\nSample chunks:\")\n",
    "    samples = chunks[:min(len(chunks), num_samples)]\n",
    "    for i, chunk in enumerate(samples, 1):\n",
    "        text = chunker.contextualize(chunk=chunk)\n",
    "        print(f\"\\nChunk {i}:\")\n",
    "        if chunker.tokenizer:\n",
    "            try:\n",
    "                count = len(chunker.tokenizer.tokenizer.encode(text))\n",
    "                print(f\"Token count: {count}\")\n",
    "            except Exception:\n",
    "                print(\"Token count: N/A\")\n",
    "        print(\"-\" * 40)\n",
    "        print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"Helper function created for analyzing chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c721",
   "metadata": {},
   "source": [
    "### 1. Default HybridChunker\n",
    "\n",
    "First, let's see how the HybridChunker with default settings behaves.\n",
    "\n",
    "> ‚ö†Ô∏è **Expected Warning**: The output may show tokenizer warnings about sequence length limits. These are informational only and do not affect the chunking results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e75003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running default HybridChunker (no tokenizer)...\")\n",
    "analyze_chunks(default_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a07aa",
   "metadata": {},
   "source": [
    "### 2. Disabling Peer Merging\n",
    "\n",
    "Let's see what happens when we disable peer merging. This will prevent the chunker from combining small adjacent chunks, even if they would fit within our token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running HybridChunker (with merge_peers=False)...\")\n",
    "analyze_chunks(no_merge_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f547fef",
   "metadata": {},
   "source": [
    "### 3. Custom Chunking\n",
    "\n",
    "Now let's set the tokenizer and max_tokens on the HybridChunker.Since we are setting max_tokens to a smaller value, you should see increased number of chunks. We are setting the merge_peers to True so this will merge chunks to stay under the token limit while trying to preserve document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running  HybridChunker with custom tokenizer and max_tokens...\")\n",
    "analyze_chunks(custom_chunker, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9703f84",
   "metadata": {},
   "source": [
    "## üç© Additional Resources\n",
    "\n",
    "For more information about Docling and its features:\n",
    "\n",
    "- [Docling Documentation](https://docling-project.github.io/docling/)\n",
    "- [Open Data Hub Data Processing Examples](https://github.com/opendatahub-io/odh-data-processing)\n",
    "\n",
    "### Any Feedback?\n",
    "\n",
    "We'd love to hear if you have any feedback on this or any other notebook in this series! Please [open an issue](https://github.com/opendatahub-io/odh-data-processing/issues) and help us improve our demos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
